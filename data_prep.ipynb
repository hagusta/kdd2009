{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "\n",
    "from datetime import datetime\n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from IPython.core.display import HTML\n",
    "import json\n",
    "import pickle, time\n",
    "\n",
    "from hyperopt import fmin, tpe, Trials, STATUS_OK, STATUS_FAIL, hp, pyll\n",
    "#from hyperopt import pyll\n",
    "import csv\n",
    "#from sklearn.ensemble import GradientBoostingClassifier as GBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization: feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(klas=1):\n",
    "    #### load data file\n",
    "    data=pd.read_csv('orange_small_test.data',header=0,delimiter='\\t')\n",
    "    #### Load label files\n",
    "    labels=dict({\n",
    "    1 : pd.read_csv('orange_small_train_appetency.labels',header=None,delimiter='\\t'),\n",
    "    2 : pd.read_csv('orange_small_train_churn.labels',header=None,delimiter='\\t'),\n",
    "    3 : pd.read_csv('orange_small_train_upselling.labels',header=None,delimiter='\\t')\n",
    "    })\n",
    "           \n",
    "    lbl=labels[klas]\n",
    "    idx=np.where(lbl!=klas)\n",
    "    lbl.at[idx[0],0]=0\n",
    "    \n",
    "    return data, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_column(col,col_name,isTrain=True):\n",
    "    new_col_name=[]\n",
    "    _row_pos=[]\n",
    "    _col_pos=[]\n",
    "    _val=[]\n",
    "    irow=0\n",
    "    if isTrain:\n",
    "        _one_hot_col_names=[]\n",
    "        for d in col.tolist():\n",
    "            if d not in _one_hot_col_names:\n",
    "                #print(col_name,d)\n",
    "                new_col_name.append(col_name+'_' + str(d))\n",
    "                _one_hot_col_names.append(d)\n",
    "            _val.append(1)\n",
    "            _row_pos.append(irow)\n",
    "            _col_pos.append(_one_hot_col_names.index(d))\n",
    "            irow=irow+1\n",
    "        \n",
    "        new_data=coo_matrix((_val, (_row_pos, [ i+1 for i in _col_pos])), shape=(len(_row_pos), len(_one_hot_col_names)+1)).toarray() \n",
    "        col=_one_hot_col_names\n",
    "    else:\n",
    "        _one_hot_col_names=col_name\n",
    "        for d in col.tolist():\n",
    "            if d not in _one_hot_col_names:\n",
    "                _col_pos.append(0)\n",
    "            else:\n",
    "                col_idx=_one_hot_col_names.index(d)\n",
    "                _col_pos.append(col_idx)\n",
    "            _row_pos.append(irow)\n",
    "            _val.append(1)\n",
    "            irow=irow+1\n",
    "        new_data=coo_matrix((_val, (_row_pos, _col_pos)), shape=(len(_row_pos), len(_one_hot_col_names))).toarray()\n",
    "        new_col_name=None\n",
    "        col=None\n",
    "    return new_data, new_col_name, col\n",
    "\n",
    "def target_based_statistics(x,y,tbs_dict=None,klas=1):\n",
    "    _idx=x.index\n",
    "    idx=y[(y==klas)].index\n",
    "    #print(idx)\n",
    "    x_w_y_eq_1=np.array(x.loc[idx])\n",
    "    cols=x.columns.tolist()\n",
    "    x=np.array(x)\n",
    "    x_matrix=np.zeros(x.shape)\n",
    "    #x_matrix=1e-6\n",
    "    tbs=dict()\n",
    "    p=np.sum(y==klas)/y.shape[0]\n",
    "    if tbs_dict is None:\n",
    "        for i in range(len(cols)):\n",
    "            col_tbs=dict()\n",
    "            for val in np.unique(x[:,i]):\n",
    "                idx=np.where(x[:,i]==val)[0]\n",
    "                idx2=(np.where(x_w_y_eq_1[:,i]==val))[0]\n",
    "                vtar=(idx2.shape[0]+p)/idx.shape[0]\n",
    "                if vtar >= 1:\n",
    "                    #print(cols[i],val,vtar,idx2.shape[0],idx.shape[0])\n",
    "                    vtar=1.0\n",
    "                x_matrix[idx,i]=vtar\n",
    "                col_tbs[val]=vtar\n",
    "            tbs[cols[i]]=col_tbs\n",
    "    else:\n",
    "        tbs=tbs_dict\n",
    "        for col in cols:\n",
    "            try:\n",
    "                cats=set(tbs[col])\n",
    "            except KeyError:\n",
    "                next\n",
    "            for cat in tbs[col]:\n",
    "                try:\n",
    "                    i=cols.index(col)\n",
    "                    idx=np.where(x[:,i]==cat)[0]\n",
    "                    #print(col,cat,tbs[col][cat],idx)\n",
    "                    if len(idx) > 0:\n",
    "                        #print(idx,i)\n",
    "                        x_matrix[idx,int(i)]=tbs[col][cat]\n",
    "                    else:\n",
    "                        print('column ', col , ' does not have values of ', cat)\n",
    "                except KeyError:\n",
    "                    next\n",
    "    x_matrix[x_matrix==0]=1e-6\n",
    "    return tbs, pd.DataFrame(x_matrix,columns=cols,index=_idx)\n",
    "\n",
    "class OneHotEncoder:\n",
    "    \n",
    "    cols = None\n",
    "    verbose = 0\n",
    "    new_cols = []\n",
    "    new_cols_flat=None\n",
    "    new_data_added=None\n",
    "    new_data = None\n",
    "    new_features = None\n",
    "    data_old = None\n",
    "    iat = None\n",
    "    \n",
    "    def __init__(self, cols=None, verbose=0):\n",
    "        self.cols=cols\n",
    "        self.verbose=verbose\n",
    "        print(\"init\")\n",
    "        \n",
    "    #def _train(self, data, isTrain=True):\n",
    "    def fit(self, data):\n",
    "        _idx=data.index\n",
    "        if type(data)!=pd.DataFrame:\n",
    "            print(\"exit data type must be: \",pd.DataFrame)\n",
    "            return None\n",
    "        \n",
    "        if self.cols == None:\n",
    "            self.cols = data.columns.tolist()\n",
    "            cols = self.cols\n",
    "        \n",
    "        cols=self.cols\n",
    "        new_col_name=dict()\n",
    "        new_col_name_flat=[]\n",
    "        new_features=[]\n",
    "        i=0\n",
    "        for col in cols:\n",
    "            _new_dat,_new_col_name,_new_features=tokenize_column(data[col],col)\n",
    "            if self.verbose:\n",
    "                print(col,_new_dat.shape)\n",
    "            if i == 0:\n",
    "                new_dat=np.array(_new_dat)\n",
    "                i=1\n",
    "            else:\n",
    "                new_dat=np.concatenate([new_dat,_new_dat],axis=1)\n",
    "            new_col_name[col]=['unk']+_new_features\n",
    "            new_col_name_flat=new_col_name_flat+[col + '_unk']+_new_col_name\n",
    "            new_features = new_features + _new_features\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('new columns len',len(new_col_name_flat),'new data shape:',new_dat.shape)\n",
    "        \n",
    "        self.data_old=data\n",
    "        self.new_col=new_col_name\n",
    "        self.new_cols_flat=new_col_name_flat\n",
    "        self.new_feature=new_features\n",
    "        new_data=pd.DataFrame(new_dat,columns=new_col_name_flat,dtype=np.int8,index=_idx)\n",
    "        #new_data=pd.DataFrame(new_dat,columns=new_col_name_flat)\n",
    "        self.new_data_added=new_data\n",
    "        self.iat=data.iat\n",
    "        return None\n",
    "    \n",
    "    def transform(self,data):\n",
    "        #print(data.iat,self.iat)\n",
    "        _idx=data.index\n",
    "        if data.iat == self.iat:\n",
    "            new_data=self.new_data_added\n",
    "        else:\n",
    "            cols=self.cols\n",
    "            _new_binarized_features=None\n",
    "            icol=0\n",
    "            for col in cols:\n",
    "                #print(self.new_col[col])\n",
    "                _new_data,_,_=tokenize_column(data[col],self.new_col[col],isTrain=False)\n",
    "                #print(_new_data)\n",
    "                if icol == 0:\n",
    "                    new_data=np.array(_new_data,dtype=np.int8)\n",
    "                    #new_data=np.array(_new_data)\n",
    "                    icol = 1\n",
    "                else:\n",
    "                    new_data=np.concatenate([new_data,_new_data],axis=1)\n",
    "            #print(new_data)\n",
    "            new_data=pd.DataFrame(new_data,columns=self.new_cols_flat,dtype=np.int8,index=_idx)\n",
    "            #new_data=pd.DataFrame(new_data,columns=self.new_cols_flat)\n",
    "            #print(_idx)\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_based_statistics(x,y,tbs_dict=None,klas=1):\n",
    "    _idx=x.index\n",
    "    idx=np.where(y==klas)[0]\n",
    "    x_w_y_eq_1=np.array(x.loc[idx])\n",
    "    cols=x.columns.tolist()\n",
    "    x=np.array(x)\n",
    "    tbs=dict()\n",
    "    p=np.sum(y==klas)[0]/y.shape[0]\n",
    "    if tbs_dict is None:\n",
    "        #cols=cols\n",
    "        x_matrix=np.zeros(x.shape)\n",
    "        for i in range(len(cols)):\n",
    "            col_tbs=dict()\n",
    "            for val in np.unique(x[:,i]):\n",
    "                idx=np.where(x[:,i]==val)[0]\n",
    "                #print(idx,val)\n",
    "                idx2=np.where(x_w_y_eq_1[:,i]==val)[0]\n",
    "                #print(idx2.shape[0],val,p)\n",
    "                d=idx.shape[0]\n",
    "                d=1e-6 if d == 0 else d\n",
    "                vtar=(idx2.shape[0]+p)/d\n",
    "                vtar=1 if vtar >=1 else vtar\n",
    "                x_matrix[idx,i]=vtar\n",
    "                col_tbs[val]=vtar\n",
    "            tbs[cols[i]]=col_tbs\n",
    "    else:\n",
    "        tbs=tbs_dict\n",
    "        cols=list(tbs.keys())\n",
    "        x_matrix=np.zeros((x.shape[0],len(tbs)))      \n",
    "        for col in cols:\n",
    "            try:\n",
    "                cats=set(tbs[col])\n",
    "            except KeyError:\n",
    "                next\n",
    "            for cat in tbs[col]:\n",
    "                try:\n",
    "                    i=cols.index(col)\n",
    "                    idx=np.where(x[:,i]==cat)[0]\n",
    "                    #print(col,cat,tbs[col][cat],idx)\n",
    "                    if len(idx) > 0:\n",
    "                        #print(idx,i)\n",
    "                        x_matrix[idx,int(i)]=tbs[col][cat]\n",
    "                    else:\n",
    "                        #print('column ', col , ' does not have values of ', cat)\n",
    "                        next\n",
    "                except KeyError:\n",
    "                    next\n",
    "    x_matrix[x_matrix==0]=1e-6\n",
    "    return tbs, pd.DataFrame(x_matrix,columns=cols,index=_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(x,y):\n",
    "    \n",
    "    #drop empty cols\n",
    "    drop_cols=[]\n",
    "    for col, isEmpty in (x.describe().T['count']==0).items():\n",
    "        if isEmpty:\n",
    "            drop_cols.append(col)\n",
    "            #print(col)\n",
    "    #x=x.drop(drop_cols,axis=1)\n",
    "    col_num=[]\n",
    "    col_cat=[]\n",
    "    for col, typ in x.dtypes.items():\n",
    "        if typ == np.object:\n",
    "            col_cat.append(col)\n",
    "        else:\n",
    "            col_num.append(col)\n",
    "    for col in col_cat:\n",
    "        x[col].fillna('?',inplace=True)       \n",
    "    \n",
    "    col_app=[]\n",
    "    \n",
    "    for col in col_num:\n",
    "        x[col +\"_imputed\"]=pd.isnull(x[col]).astype('float')\n",
    "        col_app.append(col +\"_imputed\")\n",
    "        x[col].fillna(0,inplace=True)\n",
    "    \n",
    "    #col_num=col_num+col_app\n",
    "    col_cat=col_cat+col_app\n",
    "    \n",
    "    xtrain,xtest,ytrain,ytest=train_test_split(x, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n",
    "    print('train data: ',xtrain.shape[0])\n",
    "    print('train data class: ',(ytrain==1)[0].sum())\n",
    "    print('test data: ', xtest.shape[0])\n",
    "    print('test data class: ', (ytest==1)[0].sum())\n",
    "    print('test/train class ratios:',(ytest==1)[0].sum()/(ytrain==1)[0].sum())\n",
    "    \n",
    "    xtrain_cat=xtrain.loc[:,col_cat]\n",
    "    xtrain_num=xtrain.loc[:,col_num]\n",
    "    xtest_cat=xtest.loc[:,col_cat]\n",
    "    xtest_num=xtest.loc[:,col_num]\n",
    "    \n",
    "    print('norm')\n",
    "    norm = pre.Normalizer()\n",
    "    xtrain_norm = norm.fit_transform(xtrain_num)\n",
    "    xtest_norm = norm.transform(xtest_num)\n",
    "    xtrain_norm=pd.DataFrame(xtrain_norm,columns=col_num, index=xtrain_num.index)\n",
    "    xtest_norm=pd.DataFrame(xtest_norm,columns=col_num, index=xtest_num.index)\n",
    "    \n",
    "    print('tbs')\n",
    "    # replace categories with target based statistic \n",
    "    this_tbs, xtrain_cat_tbs=target_based_statistics(xtrain_cat, ytrain)\n",
    "    _,xtest_cat_tbs=target_based_statistics(xtest_cat, ytest, tbs_dict=this_tbs)\n",
    "    \n",
    "    #xtrain_cat_tbs=[]\n",
    "    #xtest_cat_tbs=[]\n",
    "    \n",
    "    data=dict({\n",
    "        'category_columns':col_cat,\n",
    "        'numeric_columns':col_num,\n",
    "        'train_cat':xtrain_cat,\n",
    "        'test_cat':xtest_cat,\n",
    "        'train_tbs':xtrain_cat_tbs,\n",
    "        'test_tbs':xtest_cat_tbs,\n",
    "        'train_num':xtrain_num,\n",
    "        'test_num':xtest_num,\n",
    "        'train_norm':xtrain_norm,\n",
    "        'test_norm':xtest_norm,\n",
    "        'y_train':ytrain,\n",
    "        'y_test':ytest\n",
    "    }\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  45000\n",
      "train data class:  801\n",
      "test data:  5000\n",
      "test data class:  89\n",
      "test/train class ratios: 0.1111111111111111\n",
      "norm\n",
      "tbs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x,y=load_data()\n",
    "data=data_preparation(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle','wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle','rb') as f:\n",
    "    data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.concat([data['train_num'],data['train_tbs']],axis=1)\n",
    "x_test=pd.concat([data['test_num'],data['test_tbs']],axis=1)\n",
    "y_train=data['y_train']\n",
    "y_test=data['y_test']\n",
    "column_names=x_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var1\n",
      "Var2\n",
      "Var3\n",
      "Var4\n",
      "Var5\n",
      "Var6\n",
      "Var7\n",
      "Var8\n",
      "Var9\n",
      "Var10\n",
      "Var11\n",
      "Var12\n",
      "Var13\n",
      "Var14\n",
      "Var15\n",
      "Var16\n",
      "Var17\n",
      "Var18\n",
      "Var19\n",
      "Var20\n",
      "Var21\n",
      "Var22\n",
      "Var23\n",
      "Var24\n",
      "Var25\n",
      "Var26\n",
      "Var27\n",
      "Var28\n",
      "Var29\n",
      "Var30\n",
      "Var31\n",
      "Var32\n",
      "Var33\n",
      "Var34\n",
      "Var35\n",
      "Var36\n",
      "Var37\n",
      "Var38\n",
      "Var39\n",
      "Var40\n",
      "Var41\n",
      "Var42\n",
      "Var43\n",
      "Var44\n",
      "Var45\n",
      "Var46\n",
      "Var47\n",
      "Var48\n",
      "Var49\n",
      "Var50\n",
      "Var51\n",
      "Var52\n",
      "Var53\n",
      "Var54\n",
      "Var55\n",
      "Var56\n",
      "Var57\n",
      "Var58\n",
      "Var59\n",
      "Var60\n",
      "Var61\n",
      "Var62\n",
      "Var63\n",
      "Var64\n",
      "Var65\n",
      "Var66\n",
      "Var67\n",
      "Var68\n",
      "Var69\n",
      "Var70\n",
      "Var71\n",
      "Var72\n",
      "Var73\n",
      "Var74\n",
      "Var75\n",
      "Var76\n",
      "Var77\n",
      "Var78\n",
      "Var79\n",
      "Var80\n",
      "Var81\n",
      "Var82\n",
      "Var83\n",
      "Var84\n",
      "Var85\n",
      "Var86\n",
      "Var87\n",
      "Var88\n",
      "Var89\n",
      "Var90\n",
      "Var91\n",
      "Var92\n",
      "Var93\n",
      "Var94\n",
      "Var95\n",
      "Var96\n",
      "Var97\n",
      "Var98\n",
      "Var99\n",
      "Var100\n",
      "Var101\n",
      "Var102\n",
      "Var103\n",
      "Var104\n",
      "Var105\n",
      "Var106\n",
      "Var107\n",
      "Var108\n",
      "Var109\n",
      "Var110\n",
      "Var111\n",
      "Var112\n",
      "Var113\n",
      "Var114\n",
      "Var115\n",
      "Var116\n",
      "Var117\n",
      "Var118\n",
      "Var119\n",
      "Var120\n",
      "Var121\n",
      "Var122\n",
      "Var123\n",
      "Var124\n",
      "Var125\n",
      "Var126\n",
      "Var127\n",
      "Var128\n",
      "Var129\n",
      "Var130\n",
      "Var131\n",
      "Var132\n",
      "Var133\n",
      "Var134\n",
      "Var135\n",
      "Var136\n",
      "Var137\n",
      "Var138\n",
      "Var139\n",
      "Var140\n",
      "Var141\n",
      "Var142\n",
      "Var143\n",
      "Var144\n",
      "Var145\n",
      "Var146\n",
      "Var147\n",
      "Var148\n",
      "Var149\n",
      "Var150\n",
      "Var151\n",
      "Var152\n",
      "Var153\n",
      "Var154\n",
      "Var155\n",
      "Var156\n",
      "Var157\n",
      "Var158\n",
      "Var159\n",
      "Var160\n",
      "Var161\n",
      "Var162\n",
      "Var163\n",
      "Var164\n",
      "Var165\n",
      "Var166\n",
      "Var167\n",
      "Var168\n",
      "Var169\n",
      "Var170\n",
      "Var171\n",
      "Var172\n",
      "Var173\n",
      "Var174\n",
      "Var175\n",
      "Var176\n",
      "Var177\n",
      "Var178\n",
      "Var179\n",
      "Var180\n",
      "Var181\n",
      "Var182\n",
      "Var183\n",
      "Var184\n",
      "Var185\n",
      "Var186\n",
      "Var187\n",
      "Var188\n",
      "Var189\n",
      "Var190\n",
      "Var209\n",
      "Var230\n",
      "Var191\n",
      "Var192\n",
      "Var193\n",
      "Var194\n",
      "Var195\n",
      "Var196\n",
      "Var197\n",
      "Var198\n",
      "Var199\n",
      "Var200\n",
      "Var201\n",
      "Var202\n",
      "Var203\n",
      "Var204\n",
      "Var205\n",
      "Var206\n",
      "Var207\n",
      "Var208\n",
      "Var210\n",
      "Var211\n",
      "Var212\n",
      "Var213\n",
      "Var214\n",
      "Var215\n",
      "Var216\n",
      "Var217\n",
      "Var218\n",
      "Var219\n",
      "Var220\n",
      "Var221\n",
      "Var222\n",
      "Var223\n",
      "Var224\n",
      "Var225\n",
      "Var226\n",
      "Var227\n",
      "Var228\n",
      "Var229\n",
      "Var1imputed\n",
      "Var2imputed\n",
      "Var3imputed\n",
      "Var4imputed\n",
      "Var5imputed\n",
      "Var6imputed\n",
      "Var7imputed\n",
      "Var8imputed\n",
      "Var9imputed\n",
      "Var10imputed\n",
      "Var11imputed\n",
      "Var12imputed\n",
      "Var13imputed\n",
      "Var14imputed\n",
      "Var15imputed\n",
      "Var16imputed\n",
      "Var17imputed\n",
      "Var18imputed\n",
      "Var19imputed\n",
      "Var20imputed\n",
      "Var21imputed\n",
      "Var22imputed\n",
      "Var23imputed\n",
      "Var24imputed\n",
      "Var25imputed\n",
      "Var26imputed\n",
      "Var27imputed\n",
      "Var28imputed\n",
      "Var29imputed\n",
      "Var30imputed\n",
      "Var31imputed\n",
      "Var32imputed\n",
      "Var33imputed\n",
      "Var34imputed\n",
      "Var35imputed\n",
      "Var36imputed\n",
      "Var37imputed\n",
      "Var38imputed\n",
      "Var39imputed\n",
      "Var40imputed\n",
      "Var41imputed\n",
      "Var42imputed\n",
      "Var43imputed\n",
      "Var44imputed\n",
      "Var45imputed\n",
      "Var46imputed\n",
      "Var47imputed\n",
      "Var48imputed\n",
      "Var49imputed\n",
      "Var50imputed\n",
      "Var51imputed\n",
      "Var52imputed\n",
      "Var53imputed\n",
      "Var54imputed\n",
      "Var55imputed\n",
      "Var56imputed\n",
      "Var57imputed\n",
      "Var58imputed\n",
      "Var59imputed\n",
      "Var60imputed\n",
      "Var61imputed\n",
      "Var62imputed\n",
      "Var63imputed\n",
      "Var64imputed\n",
      "Var65imputed\n",
      "Var66imputed\n",
      "Var67imputed\n",
      "Var68imputed\n",
      "Var69imputed\n",
      "Var70imputed\n",
      "Var71imputed\n",
      "Var72imputed\n",
      "Var73imputed\n",
      "Var74imputed\n",
      "Var75imputed\n",
      "Var76imputed\n",
      "Var77imputed\n",
      "Var78imputed\n",
      "Var79imputed\n",
      "Var80imputed\n",
      "Var81imputed\n",
      "Var82imputed\n",
      "Var83imputed\n",
      "Var84imputed\n",
      "Var85imputed\n",
      "Var86imputed\n",
      "Var87imputed\n",
      "Var88imputed\n",
      "Var89imputed\n",
      "Var90imputed\n",
      "Var91imputed\n",
      "Var92imputed\n",
      "Var93imputed\n",
      "Var94imputed\n",
      "Var95imputed\n",
      "Var96imputed\n",
      "Var97imputed\n",
      "Var98imputed\n",
      "Var99imputed\n",
      "Var100imputed\n",
      "Var101imputed\n",
      "Var102imputed\n",
      "Var103imputed\n",
      "Var104imputed\n",
      "Var105imputed\n",
      "Var106imputed\n",
      "Var107imputed\n",
      "Var108imputed\n",
      "Var109imputed\n",
      "Var110imputed\n",
      "Var111imputed\n",
      "Var112imputed\n",
      "Var113imputed\n",
      "Var114imputed\n",
      "Var115imputed\n",
      "Var116imputed\n",
      "Var117imputed\n",
      "Var118imputed\n",
      "Var119imputed\n",
      "Var120imputed\n",
      "Var121imputed\n",
      "Var122imputed\n",
      "Var123imputed\n",
      "Var124imputed\n",
      "Var125imputed\n",
      "Var126imputed\n",
      "Var127imputed\n",
      "Var128imputed\n",
      "Var129imputed\n",
      "Var130imputed\n",
      "Var131imputed\n",
      "Var132imputed\n",
      "Var133imputed\n",
      "Var134imputed\n",
      "Var135imputed\n",
      "Var136imputed\n",
      "Var137imputed\n",
      "Var138imputed\n",
      "Var139imputed\n",
      "Var140imputed\n",
      "Var141imputed\n",
      "Var142imputed\n",
      "Var143imputed\n",
      "Var144imputed\n",
      "Var145imputed\n",
      "Var146imputed\n",
      "Var147imputed\n",
      "Var148imputed\n",
      "Var149imputed\n",
      "Var150imputed\n",
      "Var151imputed\n",
      "Var152imputed\n",
      "Var153imputed\n",
      "Var154imputed\n",
      "Var155imputed\n",
      "Var156imputed\n",
      "Var157imputed\n",
      "Var158imputed\n",
      "Var159imputed\n",
      "Var160imputed\n",
      "Var161imputed\n",
      "Var162imputed\n",
      "Var163imputed\n",
      "Var164imputed\n",
      "Var165imputed\n",
      "Var166imputed\n",
      "Var167imputed\n",
      "Var168imputed\n",
      "Var169imputed\n",
      "Var170imputed\n",
      "Var171imputed\n",
      "Var172imputed\n",
      "Var173imputed\n",
      "Var174imputed\n",
      "Var175imputed\n",
      "Var176imputed\n",
      "Var177imputed\n",
      "Var178imputed\n",
      "Var179imputed\n",
      "Var180imputed\n",
      "Var181imputed\n",
      "Var182imputed\n",
      "Var183imputed\n",
      "Var184imputed\n",
      "Var185imputed\n",
      "Var186imputed\n",
      "Var187imputed\n",
      "Var188imputed\n",
      "Var189imputed\n",
      "Var190imputed\n",
      "Var209imputed\n",
      "Var230imputed\n"
     ]
    }
   ],
   "source": [
    "_=[ print(col.replace('_','')) for col in column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu3",
   "language": "python",
   "name": "tensorflow-gpu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
