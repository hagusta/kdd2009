{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, time\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import layers,losses,metrics\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07386184859845535"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(.9288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(klas=1):\n",
    "    #### load data file\n",
    "    data=pd.read_csv('orange_small_test.data',header=0,delimiter='\\t')\n",
    "    #### Load label files\n",
    "    labels=dict({\n",
    "    1 : pd.read_csv('orange_small_train_appetency.labels',header=None,delimiter='\\t'),\n",
    "    2 : pd.read_csv('orange_small_train_churn.labels',header=None,delimiter='\\t'),\n",
    "    3 : pd.read_csv('orange_small_train_upselling.labels',header=None,delimiter='\\t')\n",
    "    })\n",
    "           \n",
    "    lbl=labels[klas]\n",
    "    idx=np.where(lbl!=klas)\n",
    "    lbl.at[idx[0],0]=0\n",
    "    \n",
    "    return data, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=[]\n",
    "empty_cols=[]\n",
    "for col, isEmpty in (x.describe().T['count']==0).items():\n",
    "    if isEmpty:\n",
    "        empty_cols.append(col)\n",
    "        #print(col)\n",
    "#x=x.drop(drop_cols,axis=1)\n",
    "col_num=[]\n",
    "col_cat=[]\n",
    "for col, typ in x.dtypes.items():\n",
    "    if typ == np.object:\n",
    "        col_cat.append(col)\n",
    "    else:\n",
    "        col_num.append(col)\n",
    "        \n",
    "for col in col_cat:\n",
    "    x[col].fillna('?',inplace=True)       \n",
    "\n",
    "col_app=[]\n",
    "\n",
    "for col in col_num:\n",
    "    x[col +\"_imputed\"]=pd.isnull(x[col]).astype('float')\n",
    "    col_app.append(col +\"_*\")\n",
    "    x[col].fillna(0,inplace=True)\n",
    "\n",
    "#col_num=col_num+col_app\n",
    "col_cat=col_cat+col_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle','rb') as f:\n",
    "    data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.concat([data['train_num'],data['train_cat']],axis=1)\n",
    "x_test=pd.concat([data['test_num'],data['test_cat']],axis=1)\n",
    "y_train=data['y_train']\n",
    "y_test=data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train=np.array(x_train).astype('float32')\n",
    "x=np.array(x_test[0:100]).astype('float32')\n",
    "#y_train=np.array(y_train[0:100,:])\n",
    "y=np.array(y_test[0:100])\n",
    "data=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.concat([x_train,x_test],axis=0)\n",
    "y=pd.concat([y_train,y_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxEvent=np.where(y==1)[0]\n",
    "idxNonEvent=np.where(y!=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var1 WoE: 0.6320961290040762\n",
      "Var2 WoE: 3.4743337869825566\n"
     ]
    }
   ],
   "source": [
    "ndata=x.shape[0]\n",
    "globalEvent=idxEvent.shape[0]\n",
    "globalNonEvent=idxNonEvent.shape[0]\n",
    "meanGlobalEvent=globalEvent/ndata\n",
    "for col in x.columns.tolist()[0:2]:\n",
    "    groupEvent=((x.loc[idxEvent])[col]!=0).sum()\n",
    "    groupNonEvent=((x.loc[idxEvent])[col]==0).sum()\n",
    "    nonEvent=ndata-groupEvent\n",
    "    print(col,'WoE:', np.log(((groupNonEvent+0.5)/globalNonEvent)/((groupEvent+0.5)/globalEvent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    890\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(4)\n",
    "    res=pool.map_async(f,range(20))\n",
    "    print(res.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3001, 40040) (3001,)\n",
      "(3001, 40040) (3001,)\n",
      "(3001, 40040) (3001,)\n",
      "(3001, 40040) (3001,)\n",
      "(2996, 40040) (2996,)\n"
     ]
    }
   ],
   "source": [
    "n_fold=5\n",
    "idx=np.where(y==1)[0]\n",
    "idxf=np.where(y!=1)[0]\n",
    "dt_t=np.concatenate((x[idx,:],y[idx,:]),axis=1)\n",
    "dt_f=np.concatenate((x[idxf,:],y[idxf,:]),axis=1)\n",
    "\n",
    "n_f=dt_f.shape[0]\n",
    "n_t=dt_t.shape[0]\n",
    "n_f_batch=int(n_f/n_fold)+1\n",
    "n_t_batch=int(n_t/n_fold)+1\n",
    "\n",
    "cnt=0\n",
    "i=0\n",
    "j=1\n",
    "col_n=dt_t.shape[1]-1\n",
    "while(j):\n",
    "    dt_batch=np.concatenate((dt_t[n_t_batch*(i):n_t_batch*(i+1)],dt_f[n_f_batch*i:n_f_batch*(i+1)]),axis=0)\n",
    "    #print(batch.shape)\n",
    "    np.random.shuffle(dt_batch)\n",
    "    x_batch=dt_batch[:,0:col_n-1]\n",
    "    y_batch=dt_batch[:,col_n]\n",
    "    print(x_batch.shape,y_batch.shape)\n",
    "    #batches.append(batch)\n",
    "    cnt=cnt+n_f_batch\n",
    "    i=i+1\n",
    "    if n_f < cnt:\n",
    "        i=0\n",
    "        j=0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu3",
   "language": "python",
   "name": "tensorflow-gpu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
