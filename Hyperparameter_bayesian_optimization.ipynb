{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "\n",
    "from datetime import datetime\n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from IPython.core.display import HTML\n",
    "import json\n",
    "import pickle, time\n",
    "\n",
    "from hyperopt import fmin, tpe, Trials, STATUS_OK, STATUS_FAIL, hp, pyll\n",
    "import csv\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization: feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle','rb') as f:\n",
    "    data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.concat([data['train_num'],data['train_tbs']],axis=1)\n",
    "x_test=pd.concat([data['test_num'],data['test_tbs']],axis=1)\n",
    "x=pd.concat([x_train,x_test],axis=0)\n",
    "y_train=data['y_train']\n",
    "y_test=data['y_test']\n",
    "y=pd.concat([y_train,y_test],axis=0)\n",
    "xE1=x.iloc[np.where(y==1)[0].tolist()]\n",
    "xdesc=xE1.describe().T\n",
    "colsZeroMean=[]\n",
    "_=[colsZeroMean.append(col) for col in xdesc.iloc[np.where((xdesc['mean']==0) & (xdesc['std'] ==0))[0]].index]\n",
    "#colsZeroMean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_x=data['train_cat']\n",
    "for col in _x.columns:\n",
    "    _x[col]=_x[col].astype('object')\n",
    "x_train=pd.concat([data['train_num'],_x],axis=1)\n",
    "_x=data['test_cat']\n",
    "for col in _x.columns:\n",
    "    _x[col]=_x[col].astype('object')\n",
    "x_test=pd.concat([data['test_num'],_x],axis=1)\n",
    "y_train=data['y_train']\n",
    "y_test=data['y_test']\n",
    "category_columns=data['test_cat'].columns.tolist()\n",
    "column_names=x_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 422)\n",
      "(45000, 394)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_train.drop(labels=colsZeroMean,axis=1,inplace=True)\n",
    "x_test.drop(labels=colsZeroMean,axis=1,inplace=True)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_all(run_time_saved):\n",
    "    fdir='cv_run/'+run_time_saved\n",
    "    files=glob(fdir+'/feature*')\n",
    "    feature_importance=dict()\n",
    "    for f in files:\n",
    "        cols=[]\n",
    "        vals=[]\n",
    "        feat_imp=dict()\n",
    "        feature=pd.read_pickle(f)\n",
    "        for key, val in feature.items():\n",
    "            cols.append(key)\n",
    "            vals.append(val)\n",
    "        feat_imp.update({'columns':cols})\n",
    "        feat_imp.update({'importance':vals})\n",
    "        cnt=f.split('.')[1]+'.'+f.split('.')[2]\n",
    "        feature_importance.update({cnt:feat_imp})\n",
    "    return feature_importance\n",
    "\n",
    "def feature_importance_run(runid):\n",
    "    global feature_importance\n",
    "    #print(feature_importance[runid]['columns'])\n",
    "    cols=feature_importance[runid]['columns']\n",
    "    vals=feature_importance[runid]['importance']\n",
    "    idx=np.where(np.array(vals)>0)\n",
    "    cols=(np.array(cols))[idx]\n",
    "    _cols=[]\n",
    "    for col in cols:\n",
    "        if '_imputed' not in col:\n",
    "            _cols.append(col)\n",
    "    return _cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters(iterations=201,\n",
    "        depth=6,\n",
    "        thread_count=8,\n",
    "        learning_rate=0.03,\n",
    "        loss_function='Logloss',\n",
    "        fold_len_multiplier = 2,\n",
    "        rsm = 1,\n",
    "        border_count = 128,\n",
    "        max_ctr_complexity = 2,\n",
    "        l2_leaf_reg = 3,\n",
    "        leaf_estimation_method = 'Newton',\n",
    "        leaf_estimation_iterations = 10,\n",
    "        random_strength = 1,\n",
    "        one_hot_max_size = 0,\n",
    "        bagging_temperature = 1,\n",
    "        logging_level = 'Verbose',\n",
    "        eval_metric = 'AUC',\n",
    "        custom_loss = ['Accuracy','Precision','Recall'],\n",
    "        random_seed=300,\n",
    "        metric_period=50,\n",
    "        calc_feature_importance = True,\n",
    "        task_type = 'CPU',\n",
    "        #class_weights = (1,4)\n",
    "              ):\n",
    "    params=dict(iterations=iterations,\n",
    "        depth=depth,\n",
    "        thread_count=thread_count,\n",
    "        learning_rate=learning_rate,\n",
    "        loss_function=loss_function,\n",
    "        fold_len_multiplier = fold_len_multiplier,\n",
    "        rsm = rsm,\n",
    "        border_count = border_count,\n",
    "        max_ctr_complexity=max_ctr_complexity,\n",
    "        l2_leaf_reg = l2_leaf_reg,\n",
    "        leaf_estimation_method = leaf_estimation_method,\n",
    "        leaf_estimation_iterations = leaf_estimation_iterations,\n",
    "        random_strength = random_strength,\n",
    "        one_hot_max_size = one_hot_max_size,\n",
    "        bagging_temperature = bagging_temperature,\n",
    "        #ctr_description = 'Borders',\n",
    "        logging_level = logging_level,\n",
    "        eval_metric = eval_metric,\n",
    "        #train_dir=strnow,\n",
    "        custom_loss = custom_loss,\n",
    "        random_seed=random_seed,\n",
    "        metric_period=metric_period,\n",
    "        calc_feature_importance = metric_period,\n",
    "        task_type=task_type,\n",
    "        #class_weights=class_weights\n",
    "               )\n",
    "    return params\n",
    "params=set_parameters(eval_metric='AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pair(x,y,n_splits=5,klas=1, cat_cols=[]):\n",
    "    #global col_cat\n",
    "    split_pair=[]\n",
    "    split_pair_raw=[]\n",
    "    split=StratifiedKFold(n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    if len(cat_cols)>0:\n",
    "        col_cat_idx=[]\n",
    "        cols=x.columns.tolist()\n",
    "        _=[ col_cat_idx.append(cols.index(col)) for col in cat_cols]\n",
    "\n",
    "    for train_idx, test_idx in split.split(x,y):\n",
    "        _x=x.iloc[train_idx]\n",
    "        _y=y.iloc[train_idx]\n",
    "        _xt=x.iloc[test_idx]\n",
    "        _yt=y.iloc[test_idx]\n",
    "        \n",
    "        if len(cat_cols) > 0:\n",
    "            trainp=Pool(_x,_y,cat_features=col_cat_idx)\n",
    "            testp=Pool(_xt,_yt,cat_features=col_cat_idx)\n",
    "        else:\n",
    "            trainp=Pool(_x,_y)\n",
    "            testp=Pool(_xt,_yt)\n",
    "        split_pair.append((trainp,testp))\n",
    "        #split_pair_raw.append(((trainx,trainy),(testx,testy)))\n",
    "    return split_pair, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'category_data': hp.choice('category',['tbs','non']),\n",
    "        'numerical_data': hp.choice('numeric',['normal','non'])\n",
    "                               }\n",
    "param_space = {\n",
    "            'depth': hp.choice('depth', [2,4,6]),\n",
    "            'border_count': hp.choice('border_count', [64,128]),\n",
    "            'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "            'random_strength': hp.choice('random_strength', [1,  20]),\n",
    "            'one_hot_max_size': hp.choice('one_hot_max_size', [0, 25]),\n",
    "            'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n",
    "            'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),\n",
    "            'leaf_estimation_iterations':hp.choice('leaf_estimation_iterations',[1,3,5,7,10]),\n",
    "            'max_ctr_complexity':hp.quniform('max_ctr_complexity',1,5,1),\n",
    "            'leaf_estimation_method':hp.choice('leaf_estimation_method',['Newton','Gradient']),\n",
    "            'class_weights': (hp.choice('non_class_weights_ratio',[1]), hp.uniform('class_weights_ratio',1,20))\n",
    "    \n",
    "        }\n",
    "\n",
    "space.update({'param_space':param_space})\n",
    "feature_space={}\n",
    "cols=data['category_columns']+data['numeric_columns']\n",
    "_=[feature_space.update({col : eval(\"hp.choice('\"+ col + \"',[0,1])\")}) for col in cols]\n",
    "\n",
    "space.update({'feature_space':feature_space})\n",
    "#space.update({'data':data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "space2={'category_data': hp.choice('category',['non']),\n",
    "        'numerical_data': hp.choice('numeric',['normal','non'])\n",
    "                               }\n",
    "param_space = {\n",
    "            'depth': hp.choice('depth', [4,6,8]),\n",
    "            'border_count': hp.choice('border_count', [64,128]),\n",
    "            'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "            'random_strength': hp.choice('random_strength', [1,  20]),\n",
    "            'one_hot_max_size': hp.choice('one_hot_max_size', [0, 25]),\n",
    "            'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n",
    "            'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),\n",
    "            'leaf_estimation_iterations':hp.choice('leaf_estimation_iterations',[1,3,5,7,10]),\n",
    "            'max_ctr_complexity':hp.quniform('max_ctr_complexity',1,5,1),\n",
    "            'leaf_estimation_method':hp.choice('leaf_estimation_method',['Newton','Gradient']),\n",
    "            'class_weights': (hp.choice('non_class_weights_ratio',[1]), hp.uniform('class_weights_ratio',1,20))\n",
    "    \n",
    "        }\n",
    "\n",
    "space2.update({'param_space':param_space})\n",
    "#feature_space={}\n",
    "#cols=data['category_columns']+data['numeric_columns']\n",
    "#_=[feature_space.update({col : eval(\"hp.choice('\"+ col + \"',[0,1])\")}) for col in cols]\n",
    "\n",
    "#space2.update({'feature_space':feature_space})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(params=None, dtrain=None, dtest=None, n_estimators=None, seed=0, run_time=None, run_cv_id=0, eval_no=0, verbose=False):\n",
    "    global metric,column_names\n",
    "    #print(run_cv_id, eval_no)\n",
    "    path=\"./cv_run/\"+str(run_time)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    fpath=path+\"/\"+str(eval_no)+\".\"+str(run_cv_id)\n",
    "    if not os.path.isdir(fpath):\n",
    "        os.mkdir(fpath)    \n",
    "    params.update({\"iterations\": n_estimators})\n",
    "    params.update({\"eval_metric\": metric})\n",
    "    params.update({\"logging_level\": 'Silent'})\n",
    "    #params.update({\"metric_period\": 100})\n",
    "    params.update({\"random_seed\": seed})\n",
    "    #params.update({\"leaf_estimation_method\": \"Newton\"})\n",
    "    #params.update({\"leaf_estimation_iterations\" : 10})\n",
    "    params.update({\"rsm\" : 1})\n",
    "    params.update({\"thread_count\" : 8})\n",
    "    params.update({\"fold_len_multiplier\": 2})\n",
    "    #params.update({\"max_ctr_complexity\":5})\n",
    "    params.update({\"train_dir\": fpath})\n",
    "    params.update({\"calc_feature_importance\" : True})\n",
    "    params.update({'od_type':'Iter'})\n",
    "    params.update({'od_wait':30})\n",
    "    \n",
    "    bst = CatBoostClassifier(**params)\n",
    "    bst.fit(dtrain, eval_set=dtest, use_best_model=True)\n",
    "    with open(fpath + \"/test_error.tsv\", \"r\") as f:\n",
    "        reader=np.array(list(csv.reader(f,delimiter='\\t'))).squeeze()\n",
    "    header=reader[0]\n",
    "    feature=dict()\n",
    "    for col, val in zip(column_names,bst.__dict__['_feature_importance']):\n",
    "        feature.update({col:val})\n",
    "    #pd.to_pickle(bst.__dict__['_feature_importance'],path+\"/feature_importance.\"+str(eval_no)+\".\"+str(run_cv_id))\n",
    "    pd.to_pickle(feature,path+\"/feature_importance.\"+str(eval_no)+\".\"+str(run_cv_id))\n",
    "    idx=(header==metric).argmax()\n",
    "    results=(reader[1:reader.shape[0],idx]).astype(np.float)\n",
    "    if metric=='AUC':\n",
    "        results=1-results\n",
    "        \n",
    "    return bst, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_each_iter(results):\n",
    "    lengs=[]\n",
    "    _=[lengs.append(len(res)) for res in results]\n",
    "    mlen=np.max(lengs)\n",
    "    #print(lengs,mlen)\n",
    "    a=[]\n",
    "    for run in results:\n",
    "        a.append((np.pad(run,(0,mlen-len(run)),'constant')).tolist())\n",
    "    x=np.array(a)\n",
    "    #print(x.shape)\n",
    "    means=[]\n",
    "    for i in range(x.shape[1]):\n",
    "        mean=0\n",
    "        count=0\n",
    "        for j in range(x.shape[0]):\n",
    "            if x[j,i] > 0:\n",
    "                count=count+1\n",
    "                mean=mean+x[j,i]\n",
    "        means.append(mean/count)\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(cv_pairs, params=None, n_est=None, verbose=False, run_time=None):\n",
    "    global default_params,n_estimators,best_loss,hyperopt_eval_num,metric,hyperopt_evals\n",
    "    params = params or default_params\n",
    "    n_estimators = n_est or n_estimators\n",
    "    #print('run_cv',hyperopt_eval_num)\n",
    "    evals_results, start_time = [], time.time()\n",
    "    _loss=[]\n",
    "    i=0\n",
    "    for dtrain, dtest in cv_pairs:\n",
    "        _, evals_result = fit(params, dtrain, dtest, n_estimators, run_time=run_time, run_cv_id=i, eval_no=hyperopt_eval_num+1)\n",
    "        #evals_results.append(np.mean(evals_result,axis=0))\n",
    "        evals_results.append(evals_result)\n",
    "        _loss.append(np.min(evals_result))\n",
    "        i=i+1\n",
    "    mean_evals_results = mean_each_iter(evals_results)\n",
    "    best_n_estimators = np.argmin(mean_evals_results) + 1\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "    cv_result = {'loss': mean_evals_results[best_n_estimators - 1] ,\n",
    "                 'best_n_estimators': best_n_estimators, \n",
    "                 'eval_time': eval_time,\n",
    "                 'status': STATUS_FAIL if np.isnan(mean_evals_results[best_n_estimators - 1]) else STATUS_OK,\n",
    "                 'params': params.copy(),\n",
    "                 'losses': _loss\n",
    "                }\n",
    "    best_loss = min(best_loss, cv_result['loss'])\n",
    "    hyperopt_eval_num += 1\n",
    "    cv_result.update({'hyperopt_eval_num': hyperopt_eval_num, 'best_loss': best_loss})\n",
    "        \n",
    "    if verbose:\n",
    "        print ('[{0}/{1}]\\teval_time={2:.2f} sec\\tcurrent_{3}={4:.6f}\\tmin_{3}={5:.6f}'.format(\n",
    "                    hyperopt_eval_num, hyperopt_evals, eval_time,\n",
    "                    metric, cv_result['loss'], best_loss))\n",
    "    return cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators=50\n",
    "hyperopt_evals=0\n",
    "metric=\"AUC\"\n",
    "evals_results=[]\n",
    "space_sample=pyll.stochastic.sample(space2)\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "params=None\n",
    "hyperopt_eval_num, best_loss = 0, np.inf\n",
    "res=run_hyper_test2(space_sample,data,run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=x_train.columns.tolist()\n",
    "cv_pair,_=split_pair(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/0]\teval_time=73.32 sec\tcurrent_AUC=0.452720\tmin_AUC=0.452720\n",
      "[2/0]\teval_time=84.26 sec\tcurrent_AUC=0.461085\tmin_AUC=0.452720\n",
      "[3/0]\teval_time=38.65 sec\tcurrent_AUC=0.471458\tmin_AUC=0.452720\n",
      "[4/0]\teval_time=55.04 sec\tcurrent_AUC=0.466392\tmin_AUC=0.452720\n",
      "[5/0]\teval_time=45.42 sec\tcurrent_AUC=0.494950\tmin_AUC=0.452720\n",
      "[6/0]\teval_time=35.32 sec\tcurrent_AUC=0.478284\tmin_AUC=0.452720\n",
      "[7/0]\teval_time=71.99 sec\tcurrent_AUC=0.473927\tmin_AUC=0.452720\n",
      "[8/0]\teval_time=73.86 sec\tcurrent_AUC=0.478626\tmin_AUC=0.452720\n",
      "[9/0]\teval_time=63.29 sec\tcurrent_AUC=0.458949\tmin_AUC=0.452720\n",
      "[10/0]\teval_time=40.66 sec\tcurrent_AUC=0.472771\tmin_AUC=0.452720\n",
      "[11/0]\teval_time=32.67 sec\tcurrent_AUC=0.475612\tmin_AUC=0.452720\n",
      "[12/0]\teval_time=39.13 sec\tcurrent_AUC=0.469635\tmin_AUC=0.452720\n",
      "[13/0]\teval_time=55.34 sec\tcurrent_AUC=0.474333\tmin_AUC=0.452720\n",
      "[14/0]\teval_time=34.98 sec\tcurrent_AUC=0.470496\tmin_AUC=0.452720\n",
      "[15/0]\teval_time=52.17 sec\tcurrent_AUC=0.466942\tmin_AUC=0.452720\n",
      "[16/0]\teval_time=60.85 sec\tcurrent_AUC=0.485472\tmin_AUC=0.452720\n",
      "[17/0]\teval_time=71.55 sec\tcurrent_AUC=0.472152\tmin_AUC=0.452720\n",
      "[18/0]\teval_time=33.89 sec\tcurrent_AUC=0.472598\tmin_AUC=0.452720\n",
      "[19/0]\teval_time=65.42 sec\tcurrent_AUC=0.469102\tmin_AUC=0.452720\n",
      "[20/0]\teval_time=51.61 sec\tcurrent_AUC=0.467635\tmin_AUC=0.452720\n"
     ]
    }
   ],
   "source": [
    "n_estimators=501\n",
    "hyperopt_evals=0\n",
    "metric=\"AUC\"\n",
    "max_evals = 20\n",
    "this_trials = Trials()\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "hyperopt_eval_num, best_loss, split_pair_data = 0, np.inf, None\n",
    "args=space2['param_space'] #or pyll.stochastic.sample(space)\n",
    "_ = fmin(fn=lambda args: run_cv(cv_pair, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n",
    "\n",
    "with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/40]\teval_time=83.25 sec\tcurrent_Logloss=0.364518\tmin_Logloss=0.364518\n",
      "[2/40]\teval_time=260.52 sec\tcurrent_Logloss=0.252645\tmin_Logloss=0.252645\n",
      "[3/40]\teval_time=144.45 sec\tcurrent_Logloss=0.414332\tmin_Logloss=0.252645\n",
      "[4/40]\teval_time=79.10 sec\tcurrent_Logloss=0.291346\tmin_Logloss=0.252645\n",
      "[5/40]\teval_time=331.28 sec\tcurrent_Logloss=0.441436\tmin_Logloss=0.252645\n",
      "[6/40]\teval_time=199.96 sec\tcurrent_Logloss=0.460638\tmin_Logloss=0.252645\n",
      "[7/40]\teval_time=61.31 sec\tcurrent_Logloss=0.538487\tmin_Logloss=0.252645\n",
      "[8/40]\teval_time=143.77 sec\tcurrent_Logloss=0.326684\tmin_Logloss=0.252645\n",
      "[9/40]\teval_time=43.22 sec\tcurrent_Logloss=0.217205\tmin_Logloss=0.217205\n",
      "[10/40]\teval_time=89.31 sec\tcurrent_Logloss=0.411586\tmin_Logloss=0.217205\n",
      "[11/40]\teval_time=135.76 sec\tcurrent_Logloss=0.530839\tmin_Logloss=0.217205\n",
      "[12/40]\teval_time=82.05 sec\tcurrent_Logloss=0.179478\tmin_Logloss=0.179478\n",
      "[13/40]\teval_time=67.70 sec\tcurrent_Logloss=0.521002\tmin_Logloss=0.179478\n",
      "[14/40]\teval_time=521.75 sec\tcurrent_Logloss=0.118846\tmin_Logloss=0.118846\n",
      "[15/40]\teval_time=64.53 sec\tcurrent_Logloss=0.406382\tmin_Logloss=0.118846\n",
      "[16/40]\teval_time=137.60 sec\tcurrent_Logloss=0.343765\tmin_Logloss=0.118846\n",
      "[17/40]\teval_time=44.06 sec\tcurrent_Logloss=0.379691\tmin_Logloss=0.118846\n",
      "[18/40]\teval_time=820.92 sec\tcurrent_Logloss=0.351587\tmin_Logloss=0.118846\n",
      "[19/40]\teval_time=124.70 sec\tcurrent_Logloss=0.215034\tmin_Logloss=0.118846\n",
      "[20/40]\teval_time=125.10 sec\tcurrent_Logloss=0.531567\tmin_Logloss=0.118846\n",
      "[21/40]\teval_time=641.32 sec\tcurrent_Logloss=0.102508\tmin_Logloss=0.102508\n",
      "[22/40]\teval_time=608.39 sec\tcurrent_Logloss=0.127513\tmin_Logloss=0.102508\n",
      "[23/40]\teval_time=601.55 sec\tcurrent_Logloss=0.577099\tmin_Logloss=0.102508\n",
      "[24/40]\teval_time=517.43 sec\tcurrent_Logloss=0.107363\tmin_Logloss=0.102508\n",
      "[25/40]\teval_time=405.68 sec\tcurrent_Logloss=0.487496\tmin_Logloss=0.102508\n",
      "[26/40]\teval_time=612.64 sec\tcurrent_Logloss=0.102166\tmin_Logloss=0.102166\n",
      "[27/40]\teval_time=612.96 sec\tcurrent_Logloss=0.273419\tmin_Logloss=0.102166\n",
      "[28/40]\teval_time=306.03 sec\tcurrent_Logloss=0.152399\tmin_Logloss=0.102166\n",
      "[29/40]\teval_time=181.37 sec\tcurrent_Logloss=0.578362\tmin_Logloss=0.102166\n",
      "[30/40]\teval_time=265.08 sec\tcurrent_Logloss=0.559989\tmin_Logloss=0.102166\n",
      "[31/40]\teval_time=158.76 sec\tcurrent_Logloss=0.485776\tmin_Logloss=0.102166\n",
      "[32/40]\teval_time=605.14 sec\tcurrent_Logloss=0.372787\tmin_Logloss=0.102166\n",
      "[33/40]\teval_time=608.81 sec\tcurrent_Logloss=0.245401\tmin_Logloss=0.102166\n",
      "[34/40]\teval_time=450.46 sec\tcurrent_Logloss=0.294397\tmin_Logloss=0.102166\n",
      "[35/40]\teval_time=240.66 sec\tcurrent_Logloss=0.169728\tmin_Logloss=0.102166\n",
      "[36/40]\teval_time=542.79 sec\tcurrent_Logloss=0.439573\tmin_Logloss=0.102166\n",
      "[37/40]\teval_time=52.06 sec\tcurrent_Logloss=0.237179\tmin_Logloss=0.102166\n",
      "[38/40]\teval_time=566.46 sec\tcurrent_Logloss=0.299713\tmin_Logloss=0.102166\n",
      "[39/40]\teval_time=201.25 sec\tcurrent_Logloss=0.471966\tmin_Logloss=0.102166\n",
      "[40/40]\teval_time=148.55 sec\tcurrent_Logloss=0.389247\tmin_Logloss=0.102166\n"
     ]
    }
   ],
   "source": [
    "n_estimators=501\n",
    "hyperopt_evals=40\n",
    "metric=\"Logloss\"\n",
    "max_evals = 40\n",
    "this_trials = Trials()\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "hyperopt_eval_num, best_loss, split_pair_data = 0, np.inf, None\n",
    "args=space2['param_space'] #or pyll.stochastic.sample(space)\n",
    "_ = fmin(fn=lambda args: run_cv(cv_pair, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n",
    "\n",
    "with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('this_trials.'+run_time+'.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_params=this_trials.best_trial['result']['params']\n",
    "cols=x_train.columns.tolist()\n",
    "trainp=Pool(x_train,label=y_train)\n",
    "testp=Pool(x_test,label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "model,result=fit(params=_params,dtrain=trainp,dtest=testp,n_estimators=2001,run_time=run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.predict(trainp)\n",
    "metrics.accuracy_score(y_train,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.predict(testp)\n",
    "metrics.accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08956276933429051\n"
     ]
    }
   ],
   "source": [
    "res=model.eval_metrics(testp,['Logloss','AUC'])\n",
    "loss=res['Logloss'][(len(res['Logloss'])-1)]\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(testp,ntree_start=0,ntree_end=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred!=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model.catboost.\"+run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def confusion_mat(y_test,y_pred):\n",
    "    true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_test,y_pred).ravel()\n",
    "    print(\"Actual\\t\\t|  Pos     |   Neg    |\")\n",
    "    print(\"Predicted\\t-----------------\")\n",
    "    print(\"Pos\\t\\t| {0:08d} | {1:08d} |\".format(true_positive,false_positive))\n",
    "    print(\"Neg\\t\\t| {0:08d} | {1:08d} |\\n\".format(false_negative,true_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual\t\t|  Pos     |   Neg    |\n",
      "Predicted\t-----------------\n",
      "Pos\t\t| 00000000 | 00000000 |\n",
      "Neg\t\t| 00000089 | 00004911 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_mat(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_col(col,x,x_test,y_test,model):\n",
    "    #global x,x_test,model\n",
    "    x_col=x[col]\n",
    "    _col=x_col.sample(frac=1.0,replace=True).values\n",
    "    x[col]=_col\n",
    "    xpool=None\n",
    "    xpool=Pool(x,y_test)\n",
    "    res=model.eval_metrics(xpool,['Logloss'])\n",
    "    x[col]=x_test[col]    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_pertubation(model,x_test,y_test):\n",
    "    xtestp=Pool(x_test,y_test)\n",
    "    res=model.eval_metrics(xtestp,['Logloss'])\n",
    "    min_loss=np.min(res['Logloss'])\n",
    "    min_loss_arg=np.argmin(res['Logloss'])\n",
    "    #print(min_loss,min_loss_arg)\n",
    "    x=x_test.copy()\n",
    "    loss_gains=[]\n",
    "    for col in x_test.columns:\n",
    "        _res=evaluate_col(col,x,x_test,y_test,model)\n",
    "        _loss=_res['Logloss'][(len(_res['Logloss'])-1)]\n",
    "        _min_loss=np.min(_res['Logloss'])\n",
    "        _min_loss_arg=np.argmin(_res['Logloss'])\n",
    "        #print(col,loss_gain,_min_loss,_min_loss_arg,_min_loss/min_loss)\n",
    "        loss_gains.append([col,_min_loss,_min_loss/min_loss,(_min_loss/min_loss)-1])\n",
    "    pd_loss_gains=pd.DataFrame(loss_gains,columns=['Feature','Loss','ratio','gains'],index=x_test.columns)\n",
    "    return pd_loss_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=column_pertubation(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Loss</th>\n",
       "      <th>ratio</th>\n",
       "      <th>gains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Var1</th>\n",
       "      <td>Var1</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>-0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var3</th>\n",
       "      <td>Var3</td>\n",
       "      <td>0.090038</td>\n",
       "      <td>1.000021</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var4</th>\n",
       "      <td>Var4</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var5</th>\n",
       "      <td>Var5</td>\n",
       "      <td>0.090050</td>\n",
       "      <td>1.000148</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6</th>\n",
       "      <td>Var6</td>\n",
       "      <td>0.090039</td>\n",
       "      <td>1.000031</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature      Loss     ratio     gains\n",
       "Var1    Var1  0.090030  0.999935 -0.000065\n",
       "Var3    Var3  0.090038  1.000021  0.000021\n",
       "Var4    Var4  0.090036  0.999998 -0.000002\n",
       "Var5    Var5  0.090050  1.000148  0.000148\n",
       "Var6    Var6  0.090039  1.000031  0.000031"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses['ratio']=losses['Loss']/np.max(losses['Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Loss</th>\n",
       "      <th>ratio</th>\n",
       "      <th>gains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Var123_imputed</th>\n",
       "      <td>Var123_imputed</td>\n",
       "      <td>0.090117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var21_imputed</th>\n",
       "      <td>Var21_imputed</td>\n",
       "      <td>0.090101</td>\n",
       "      <td>0.999829</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var25_imputed</th>\n",
       "      <td>Var25_imputed</td>\n",
       "      <td>0.090101</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var78_imputed</th>\n",
       "      <td>Var78_imputed</td>\n",
       "      <td>0.090086</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var81</th>\n",
       "      <td>Var81</td>\n",
       "      <td>0.090086</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var28_imputed</th>\n",
       "      <td>Var28_imputed</td>\n",
       "      <td>0.090084</td>\n",
       "      <td>0.999632</td>\n",
       "      <td>0.000526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var132_imputed</th>\n",
       "      <td>Var132_imputed</td>\n",
       "      <td>0.090080</td>\n",
       "      <td>0.999592</td>\n",
       "      <td>0.000486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var74_imputed</th>\n",
       "      <td>Var74_imputed</td>\n",
       "      <td>0.090077</td>\n",
       "      <td>0.999554</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var35_imputed</th>\n",
       "      <td>Var35_imputed</td>\n",
       "      <td>0.090075</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var72_imputed</th>\n",
       "      <td>Var72_imputed</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.999530</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var140_imputed</th>\n",
       "      <td>Var140_imputed</td>\n",
       "      <td>0.090073</td>\n",
       "      <td>0.999518</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var22_imputed</th>\n",
       "      <td>Var22_imputed</td>\n",
       "      <td>0.090073</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var65_imputed</th>\n",
       "      <td>Var65_imputed</td>\n",
       "      <td>0.090072</td>\n",
       "      <td>0.999505</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var41_imputed</th>\n",
       "      <td>Var41_imputed</td>\n",
       "      <td>0.090070</td>\n",
       "      <td>0.999481</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var163_imputed</th>\n",
       "      <td>Var163_imputed</td>\n",
       "      <td>0.090068</td>\n",
       "      <td>0.999462</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var144_imputed</th>\n",
       "      <td>Var144_imputed</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var81_imputed</th>\n",
       "      <td>Var81_imputed</td>\n",
       "      <td>0.090065</td>\n",
       "      <td>0.999423</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var18_imputed</th>\n",
       "      <td>Var18_imputed</td>\n",
       "      <td>0.090060</td>\n",
       "      <td>0.999371</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var71_imputed</th>\n",
       "      <td>Var71_imputed</td>\n",
       "      <td>0.090059</td>\n",
       "      <td>0.999365</td>\n",
       "      <td>0.000259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var100_imputed</th>\n",
       "      <td>Var100_imputed</td>\n",
       "      <td>0.090059</td>\n",
       "      <td>0.999359</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var108_imputed</th>\n",
       "      <td>Var108_imputed</td>\n",
       "      <td>0.090059</td>\n",
       "      <td>0.999359</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var109_imputed</th>\n",
       "      <td>Var109_imputed</td>\n",
       "      <td>0.090059</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var145_imputed</th>\n",
       "      <td>Var145_imputed</td>\n",
       "      <td>0.090059</td>\n",
       "      <td>0.999355</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var112_imputed</th>\n",
       "      <td>Var112_imputed</td>\n",
       "      <td>0.090058</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var137_imputed</th>\n",
       "      <td>Var137_imputed</td>\n",
       "      <td>0.090058</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var77_imputed</th>\n",
       "      <td>Var77_imputed</td>\n",
       "      <td>0.090057</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6_imputed</th>\n",
       "      <td>Var6_imputed</td>\n",
       "      <td>0.090057</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var119_imputed</th>\n",
       "      <td>Var119_imputed</td>\n",
       "      <td>0.090056</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var94_imputed</th>\n",
       "      <td>Var94_imputed</td>\n",
       "      <td>0.090056</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var50_imputed</th>\n",
       "      <td>Var50_imputed</td>\n",
       "      <td>0.090056</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var61_imputed</th>\n",
       "      <td>Var61_imputed</td>\n",
       "      <td>0.090032</td>\n",
       "      <td>0.999054</td>\n",
       "      <td>-0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var189_imputed</th>\n",
       "      <td>Var189_imputed</td>\n",
       "      <td>0.090031</td>\n",
       "      <td>0.999053</td>\n",
       "      <td>-0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var101_imputed</th>\n",
       "      <td>Var101_imputed</td>\n",
       "      <td>0.090031</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>-0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var202</th>\n",
       "      <td>Var202</td>\n",
       "      <td>0.090031</td>\n",
       "      <td>0.999049</td>\n",
       "      <td>-0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var221</th>\n",
       "      <td>Var221</td>\n",
       "      <td>0.090031</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>-0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var36_imputed</th>\n",
       "      <td>Var36_imputed</td>\n",
       "      <td>0.090031</td>\n",
       "      <td>0.999045</td>\n",
       "      <td>-0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var150_imputed</th>\n",
       "      <td>Var150_imputed</td>\n",
       "      <td>0.090031</td>\n",
       "      <td>0.999044</td>\n",
       "      <td>-0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var1</th>\n",
       "      <td>Var1</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.999042</td>\n",
       "      <td>-0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var225</th>\n",
       "      <td>Var225</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.999037</td>\n",
       "      <td>-0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var190_imputed</th>\n",
       "      <td>Var190_imputed</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>-0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var151_imputed</th>\n",
       "      <td>Var151_imputed</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.999034</td>\n",
       "      <td>-0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var26_imputed</th>\n",
       "      <td>Var26_imputed</td>\n",
       "      <td>0.090029</td>\n",
       "      <td>0.999032</td>\n",
       "      <td>-0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var14_imputed</th>\n",
       "      <td>Var14_imputed</td>\n",
       "      <td>0.090029</td>\n",
       "      <td>0.999031</td>\n",
       "      <td>-0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var227</th>\n",
       "      <td>Var227</td>\n",
       "      <td>0.090029</td>\n",
       "      <td>0.999030</td>\n",
       "      <td>-0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var139</th>\n",
       "      <td>Var139</td>\n",
       "      <td>0.090028</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>-0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var127_imputed</th>\n",
       "      <td>Var127_imputed</td>\n",
       "      <td>0.090028</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>-0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var130_imputed</th>\n",
       "      <td>Var130_imputed</td>\n",
       "      <td>0.090028</td>\n",
       "      <td>0.999011</td>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var16_imputed</th>\n",
       "      <td>Var16_imputed</td>\n",
       "      <td>0.090027</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>-0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var33</th>\n",
       "      <td>Var33</td>\n",
       "      <td>0.090027</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>-0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var128_imputed</th>\n",
       "      <td>Var128_imputed</td>\n",
       "      <td>0.090026</td>\n",
       "      <td>0.998997</td>\n",
       "      <td>-0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var184</th>\n",
       "      <td>Var184</td>\n",
       "      <td>0.090025</td>\n",
       "      <td>0.998986</td>\n",
       "      <td>-0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var158_imputed</th>\n",
       "      <td>Var158_imputed</td>\n",
       "      <td>0.090025</td>\n",
       "      <td>0.998977</td>\n",
       "      <td>-0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var113</th>\n",
       "      <td>Var113</td>\n",
       "      <td>0.090024</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>-0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var201</th>\n",
       "      <td>Var201</td>\n",
       "      <td>0.090023</td>\n",
       "      <td>0.998957</td>\n",
       "      <td>-0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var163</th>\n",
       "      <td>Var163</td>\n",
       "      <td>0.090020</td>\n",
       "      <td>0.998928</td>\n",
       "      <td>-0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var97_imputed</th>\n",
       "      <td>Var97_imputed</td>\n",
       "      <td>0.090015</td>\n",
       "      <td>0.998874</td>\n",
       "      <td>-0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var19_imputed</th>\n",
       "      <td>Var19_imputed</td>\n",
       "      <td>0.090012</td>\n",
       "      <td>0.998843</td>\n",
       "      <td>-0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var136</th>\n",
       "      <td>Var136</td>\n",
       "      <td>0.090012</td>\n",
       "      <td>0.998835</td>\n",
       "      <td>-0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var17_imputed</th>\n",
       "      <td>Var17_imputed</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.998832</td>\n",
       "      <td>-0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var91_imputed</th>\n",
       "      <td>Var91_imputed</td>\n",
       "      <td>0.090009</td>\n",
       "      <td>0.998808</td>\n",
       "      <td>-0.000298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature      Loss     ratio     gains\n",
       "Var123_imputed  Var123_imputed  0.090117  1.000000  0.000894\n",
       "Var21_imputed    Var21_imputed  0.090101  0.999829  0.000724\n",
       "Var25_imputed    Var25_imputed  0.090101  0.999821  0.000715\n",
       "Var78_imputed    Var78_imputed  0.090086  0.999657  0.000551\n",
       "Var81                    Var81  0.090086  0.999655  0.000549\n",
       "Var28_imputed    Var28_imputed  0.090084  0.999632  0.000526\n",
       "Var132_imputed  Var132_imputed  0.090080  0.999592  0.000486\n",
       "Var74_imputed    Var74_imputed  0.090077  0.999554  0.000449\n",
       "Var35_imputed    Var35_imputed  0.090075  0.999542  0.000436\n",
       "Var72_imputed    Var72_imputed  0.090074  0.999530  0.000424\n",
       "Var140_imputed  Var140_imputed  0.090073  0.999518  0.000412\n",
       "Var22_imputed    Var22_imputed  0.090073  0.999517  0.000411\n",
       "Var65_imputed    Var65_imputed  0.090072  0.999505  0.000399\n",
       "Var41_imputed    Var41_imputed  0.090070  0.999481  0.000375\n",
       "Var163_imputed  Var163_imputed  0.090068  0.999462  0.000355\n",
       "Var144_imputed  Var144_imputed  0.090066  0.999432  0.000326\n",
       "Var81_imputed    Var81_imputed  0.090065  0.999423  0.000317\n",
       "Var18_imputed    Var18_imputed  0.090060  0.999371  0.000265\n",
       "Var71_imputed    Var71_imputed  0.090059  0.999365  0.000259\n",
       "Var100_imputed  Var100_imputed  0.090059  0.999359  0.000253\n",
       "Var108_imputed  Var108_imputed  0.090059  0.999359  0.000253\n",
       "Var109_imputed  Var109_imputed  0.090059  0.999356  0.000250\n",
       "Var145_imputed  Var145_imputed  0.090059  0.999355  0.000249\n",
       "Var112_imputed  Var112_imputed  0.090058  0.999348  0.000242\n",
       "Var137_imputed  Var137_imputed  0.090058  0.999346  0.000240\n",
       "Var77_imputed    Var77_imputed  0.090057  0.999332  0.000226\n",
       "Var6_imputed      Var6_imputed  0.090057  0.999332  0.000226\n",
       "Var119_imputed  Var119_imputed  0.090056  0.999330  0.000223\n",
       "Var94_imputed    Var94_imputed  0.090056  0.999328  0.000222\n",
       "Var50_imputed    Var50_imputed  0.090056  0.999327  0.000221\n",
       "...                        ...       ...       ...       ...\n",
       "Var61_imputed    Var61_imputed  0.090032  0.999054 -0.000052\n",
       "Var189_imputed  Var189_imputed  0.090031  0.999053 -0.000054\n",
       "Var101_imputed  Var101_imputed  0.090031  0.999049 -0.000057\n",
       "Var202                  Var202  0.090031  0.999049 -0.000058\n",
       "Var221                  Var221  0.090031  0.999046 -0.000061\n",
       "Var36_imputed    Var36_imputed  0.090031  0.999045 -0.000062\n",
       "Var150_imputed  Var150_imputed  0.090031  0.999044 -0.000063\n",
       "Var1                      Var1  0.090030  0.999042 -0.000065\n",
       "Var225                  Var225  0.090030  0.999037 -0.000069\n",
       "Var190_imputed  Var190_imputed  0.090030  0.999036 -0.000071\n",
       "Var151_imputed  Var151_imputed  0.090030  0.999034 -0.000073\n",
       "Var26_imputed    Var26_imputed  0.090029  0.999032 -0.000075\n",
       "Var14_imputed    Var14_imputed  0.090029  0.999031 -0.000076\n",
       "Var227                  Var227  0.090029  0.999030 -0.000076\n",
       "Var139                  Var139  0.090028  0.999016 -0.000091\n",
       "Var127_imputed  Var127_imputed  0.090028  0.999016 -0.000091\n",
       "Var130_imputed  Var130_imputed  0.090028  0.999011 -0.000095\n",
       "Var16_imputed    Var16_imputed  0.090027  0.999008 -0.000099\n",
       "Var33                    Var33  0.090027  0.999008 -0.000099\n",
       "Var128_imputed  Var128_imputed  0.090026  0.998997 -0.000110\n",
       "Var184                  Var184  0.090025  0.998986 -0.000120\n",
       "Var158_imputed  Var158_imputed  0.090025  0.998977 -0.000129\n",
       "Var113                  Var113  0.090024  0.998971 -0.000136\n",
       "Var201                  Var201  0.090023  0.998957 -0.000149\n",
       "Var163                  Var163  0.090020  0.998928 -0.000178\n",
       "Var97_imputed    Var97_imputed  0.090015  0.998874 -0.000233\n",
       "Var19_imputed    Var19_imputed  0.090012  0.998843 -0.000263\n",
       "Var136                  Var136  0.090012  0.998835 -0.000272\n",
       "Var17_imputed    Var17_imputed  0.090011  0.998832 -0.000275\n",
       "Var91_imputed    Var91_imputed  0.090009  0.998808 -0.000298\n",
       "\n",
       "[394 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.sort_values(['ratio'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_selected_colums(selected_col,x_train=x_train,x_test=x_test,y_train=y_train,y_test=y_test):\n",
    "    x_train2=x_train[selected_col]\n",
    "    x_test2=x_test[selected_col] \n",
    "    cv_pair,_=split_pair(x_train2,y_train)\n",
    "    n_estimators=501\n",
    "    hyperopt_evals=20\n",
    "    metric=\"AUC\"\n",
    "    max_evals = 20\n",
    "    this_trials = Trials()\n",
    "    run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "    args=space2['param_space'] #or pyll.stochastic.sample(space)\n",
    "    _ = fmin(fn=lambda args: run_cv(cv_pair, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n",
    "    with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "        pickle.dump(this_trials,f)\n",
    "    return this_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col=(losses[np.abs(losses['gains'])>0]).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/0]\teval_time=107.16 sec\tcurrent_Logloss=0.505585\tmin_Logloss=0.112904\n",
      "[22/0]\teval_time=8.98 sec\tcurrent_Logloss=0.559469\tmin_Logloss=0.112904\n",
      "[23/0]\teval_time=95.07 sec\tcurrent_Logloss=0.545702\tmin_Logloss=0.112904\n",
      "[24/0]\teval_time=50.71 sec\tcurrent_Logloss=0.560086\tmin_Logloss=0.112904\n",
      "[25/0]\teval_time=42.85 sec\tcurrent_Logloss=0.561631\tmin_Logloss=0.112904\n",
      "[26/0]\teval_time=100.30 sec\tcurrent_Logloss=0.429839\tmin_Logloss=0.112904\n",
      "[27/0]\teval_time=94.25 sec\tcurrent_Logloss=0.419326\tmin_Logloss=0.112904\n",
      "[28/0]\teval_time=17.30 sec\tcurrent_Logloss=0.125615\tmin_Logloss=0.112904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-eff402c3b228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrian_selected_colums\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselected_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-98119e403e3d>\u001b[0m in \u001b[0;36mtrian_selected_colums\u001b[1;34m(selected_col, x_train, x_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspace2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'param_space'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#or pyll.stochastic.sample(space)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     _ = fmin(fn=lambda args: run_cv(cv_pair, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n\u001b[1;32m---> 13\u001b[1;33m          space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./cv_run/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrun_time\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/trails.pickle'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_trials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-98119e403e3d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mrun_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y%m%d%H%M'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspace2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'param_space'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#or pyll.stochastic.sample(space)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     _ = fmin(fn=lambda args: run_cv(cv_pair, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n\u001b[0m\u001b[0;32m     13\u001b[0m          space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./cv_run/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrun_time\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/trails.pickle'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0653f70f6dfe>\u001b[0m in \u001b[0;36mrun_cv\u001b[1;34m(cv_pairs, params, n_est, verbose, run_time)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv_pairs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_cv_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_no\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperopt_eval_num\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m#evals_results.append(np.mean(evals_result,axis=0))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mevals_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-c5cb14382eb5>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(params, dtrain, dtest, n_estimators, seed, run_time, run_cv_id, eval_no, verbose)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mbst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/test_error.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mreader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval)\u001b[0m\n\u001b[0;32m   1631\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \"\"\"\n\u001b[1;32m-> 1633\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1634\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, pairs, sample_weight, group_id, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcalc_feature_importance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoostBase._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials=trian_selected_colums(selected_col=selected_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2=x_train[selected_col]\n",
    "x_test2=x_test[selected_col] \n",
    "cv_pair,_=split_pair(x_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20]\teval_time=6.76 sec\tcurrent_AUC=0.491687\tmin_AUC=0.491687\n",
      "[2/20]\teval_time=11.70 sec\tcurrent_AUC=0.498515\tmin_AUC=0.491687\n",
      "[3/20]\teval_time=7.20 sec\tcurrent_AUC=0.492917\tmin_AUC=0.491687\n",
      "[4/20]\teval_time=8.94 sec\tcurrent_AUC=0.498470\tmin_AUC=0.491687\n",
      "[5/20]\teval_time=20.71 sec\tcurrent_AUC=0.462522\tmin_AUC=0.462522\n",
      "[6/20]\teval_time=13.14 sec\tcurrent_AUC=0.494547\tmin_AUC=0.462522\n",
      "[7/20]\teval_time=8.80 sec\tcurrent_AUC=0.497130\tmin_AUC=0.462522\n",
      "[8/20]\teval_time=9.06 sec\tcurrent_AUC=0.492496\tmin_AUC=0.462522\n",
      "[9/20]\teval_time=13.61 sec\tcurrent_AUC=0.472812\tmin_AUC=0.462522\n",
      "[10/20]\teval_time=12.74 sec\tcurrent_AUC=0.492047\tmin_AUC=0.462522\n",
      "[11/20]\teval_time=8.10 sec\tcurrent_AUC=0.493948\tmin_AUC=0.462522\n",
      "[12/20]\teval_time=7.26 sec\tcurrent_AUC=0.492955\tmin_AUC=0.462522\n",
      "[13/20]\teval_time=12.44 sec\tcurrent_AUC=0.497207\tmin_AUC=0.462522\n",
      "[14/20]\teval_time=12.56 sec\tcurrent_AUC=0.480586\tmin_AUC=0.462522\n",
      "[15/20]\teval_time=10.46 sec\tcurrent_AUC=0.491292\tmin_AUC=0.462522\n",
      "[16/20]\teval_time=10.24 sec\tcurrent_AUC=0.489633\tmin_AUC=0.462522\n",
      "[17/20]\teval_time=8.67 sec\tcurrent_AUC=0.492785\tmin_AUC=0.462522\n",
      "[18/20]\teval_time=6.80 sec\tcurrent_AUC=0.492559\tmin_AUC=0.462522\n",
      "[19/20]\teval_time=14.55 sec\tcurrent_AUC=0.490671\tmin_AUC=0.462522\n",
      "[20/20]\teval_time=7.90 sec\tcurrent_AUC=0.491919\tmin_AUC=0.462522\n"
     ]
    }
   ],
   "source": [
    "n_estimators=501\n",
    "hyperopt_evals=20\n",
    "metric=\"AUC\"\n",
    "max_evals = 20\n",
    "this_trials = Trials()\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "hyperopt_eval_num, best_loss, split_pair_data = 0, np.inf, None\n",
    "args=space2['param_space'] #or pyll.stochastic.sample(space)\n",
    "_ = fmin(fn=lambda args: run_cv(cv_pair, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n",
    "\n",
    "with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20]\teval_time=195.17 sec\tcurrent_Logloss=0.404364\tmin_Logloss=0.404364\n",
      "[2/20]\teval_time=12.39 sec\tcurrent_Logloss=0.440029\tmin_Logloss=0.404364\n",
      "[3/20]\teval_time=37.90 sec\tcurrent_Logloss=0.096958\tmin_Logloss=0.096958\n",
      "[4/20]\teval_time=12.85 sec\tcurrent_Logloss=0.437329\tmin_Logloss=0.096958\n",
      "[5/20]\teval_time=50.45 sec\tcurrent_Logloss=0.489967\tmin_Logloss=0.096958\n",
      "[6/20]\teval_time=30.28 sec\tcurrent_Logloss=0.508005\tmin_Logloss=0.096958\n",
      "[7/20]\teval_time=201.38 sec\tcurrent_Logloss=0.453918\tmin_Logloss=0.096958\n",
      "[8/20]\teval_time=26.72 sec\tcurrent_Logloss=0.322000\tmin_Logloss=0.096958\n",
      "[9/20]\teval_time=112.61 sec\tcurrent_Logloss=0.532338\tmin_Logloss=0.096958\n",
      "[10/20]\teval_time=200.88 sec\tcurrent_Logloss=0.257821\tmin_Logloss=0.096958\n",
      "[11/20]\teval_time=3677.57 sec\tcurrent_Logloss=0.421811\tmin_Logloss=0.096958\n",
      "[12/20]\teval_time=77.52 sec\tcurrent_Logloss=0.235448\tmin_Logloss=0.096958\n",
      "[13/20]\teval_time=55.51 sec\tcurrent_Logloss=0.425873\tmin_Logloss=0.096958\n",
      "[14/20]\teval_time=27.10 sec\tcurrent_Logloss=0.257850\tmin_Logloss=0.096958\n",
      "[15/20]\teval_time=8.74 sec\tcurrent_Logloss=0.401677\tmin_Logloss=0.096958\n",
      "[16/20]\teval_time=11.06 sec\tcurrent_Logloss=0.225006\tmin_Logloss=0.096958\n",
      "[17/20]\teval_time=26.80 sec\tcurrent_Logloss=0.531948\tmin_Logloss=0.096958\n",
      "[18/20]\teval_time=59.95 sec\tcurrent_Logloss=0.569270\tmin_Logloss=0.096958\n",
      "[19/20]\teval_time=9.57 sec\tcurrent_Logloss=0.560330\tmin_Logloss=0.096958\n",
      "[20/20]\teval_time=23.79 sec\tcurrent_Logloss=0.510739\tmin_Logloss=0.096958\n",
      "[21/20]\teval_time=8.69 sec\tcurrent_Logloss=0.103706\tmin_Logloss=0.096958\n",
      "[22/20]\teval_time=8.47 sec\tcurrent_Logloss=0.111820\tmin_Logloss=0.096958\n",
      "[23/20]\teval_time=15.24 sec\tcurrent_Logloss=0.092023\tmin_Logloss=0.092023\n",
      "[24/20]\teval_time=20.37 sec\tcurrent_Logloss=0.342562\tmin_Logloss=0.092023\n",
      "[25/20]\teval_time=12.93 sec\tcurrent_Logloss=0.154796\tmin_Logloss=0.092023\n",
      "[26/20]\teval_time=24.85 sec\tcurrent_Logloss=0.335768\tmin_Logloss=0.092023\n",
      "[27/20]\teval_time=11.21 sec\tcurrent_Logloss=0.174223\tmin_Logloss=0.092023\n",
      "[28/20]\teval_time=14.80 sec\tcurrent_Logloss=0.297639\tmin_Logloss=0.092023\n",
      "[29/20]\teval_time=219.04 sec\tcurrent_Logloss=0.385774\tmin_Logloss=0.092023\n",
      "[30/20]\teval_time=57.70 sec\tcurrent_Logloss=0.098177\tmin_Logloss=0.092023\n",
      "[31/20]\teval_time=20.43 sec\tcurrent_Logloss=0.374006\tmin_Logloss=0.092023\n",
      "[32/20]\teval_time=18.59 sec\tcurrent_Logloss=0.463111\tmin_Logloss=0.092023\n",
      "[33/20]\teval_time=72.39 sec\tcurrent_Logloss=0.197841\tmin_Logloss=0.092023\n",
      "[34/20]\teval_time=26.75 sec\tcurrent_Logloss=0.295503\tmin_Logloss=0.092023\n",
      "[35/20]\teval_time=26.76 sec\tcurrent_Logloss=0.360265\tmin_Logloss=0.092023\n",
      "[36/20]\teval_time=24.97 sec\tcurrent_Logloss=0.300132\tmin_Logloss=0.092023\n",
      "[37/20]\teval_time=82.77 sec\tcurrent_Logloss=0.471765\tmin_Logloss=0.092023\n",
      "[38/20]\teval_time=17.11 sec\tcurrent_Logloss=0.140368\tmin_Logloss=0.092023\n",
      "[39/20]\teval_time=177.88 sec\tcurrent_Logloss=0.193761\tmin_Logloss=0.092023\n",
      "[40/20]\teval_time=202.16 sec\tcurrent_Logloss=0.261093\tmin_Logloss=0.092023\n"
     ]
    }
   ],
   "source": [
    "n_estimators=1001\n",
    "hyperopt_evals=20\n",
    "metric=\"Logloss\"\n",
    "max_evals = 40\n",
    "this_trials2 = Trials()\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "hyperopt_eval_num, best_loss, split_pair_data = 0, np.inf, None\n",
    "args=space2['param_space'] #or pyll.stochastic.sample(space)\n",
    "_ = fmin(fn=lambda args: run_cv(cv_pair, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials2)\n",
    "\n",
    "with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logloss': [0.4277268230795688, 0.29442362808665784, 0.22307939896450266, 0.18186405446467332, 0.15648776506661224, 0.13989948882479755, 0.12853733589109825, 0.12047655908336069, 0.11456271095522397, 0.11011733848264678, 0.10670143179513561, 0.10402875442284855, 0.10186404548328769, 0.10014704621393915, 0.09871871648809197, 0.09756932346620702, 0.0965994948395265, 0.09576000741327383, 0.09508566344910202, 0.09450301161974223, 0.09400612462671709, 0.09358681337512492, 0.09321050268288002, 0.09288280613599796, 0.0926007226860743, 0.09234910660464656, 0.09213137268000753, 0.09193280493905268, 0.09176196619751804, 0.09160999522118801, 0.09145637155847995, 0.09133174625512898, 0.09119214547172205, 0.09107846084072141, 0.0909752652403107, 0.09084001564932377, 0.09078442219395731, 0.0907200535966977, 0.09065668033448582, 0.09058010600070268, 0.09054382649936736, 0.09050459688860228, 0.09046353178143236, 0.09042350727262574, 0.09038351131655889, 0.09037542775762483, 0.09037043102800046, 0.0902952432497366, 0.0902730915353189, 0.09018528788152298, 0.09017854112962943, 0.09013413230455902, 0.09012776964276184, 0.09005927778512172, 0.09000648847452848, 0.09002269497571312, 0.09003371146592996, 0.09003987477249256, 0.09004949653903672, 0.09000036930843006, 0.09001756436012116, 0.09000973245928097, 0.08997543061110583, 0.08996550721947101, 0.08989062925314681, 0.08986440803006837, 0.08986272843242422, 0.0898892182235981, 0.08990144217600218, 0.08988510695537869, 0.08991848993023309, 0.08995043674319589, 0.08996537192372143, 0.08992341099700758, 0.08990056780936695, 0.08988221268840427, 0.08987984417312368, 0.08983248879235149, 0.0898708478634909, 0.08988529159698856, 0.08990183327218869, 0.0897563676875628], 'AUC': [0.5378009467396054, 0.5217821034641335, 0.5424831666586589, 0.5492050636155019, 0.5537854712763597, 0.515596722789244, 0.5215395843771949, 0.5273200039352154, 0.5208166029482084, 0.5087604300366753, 0.5115860061911004, 0.5109179347440622, 0.5171216187462678, 0.5160783290892493, 0.5170312460676445, 0.5160039718220276, 0.5167303851248859, 0.5173035080614717, 0.5074094156891545, 0.5068145575513808, 0.5057277974919866, 0.49542531212892865, 0.49618261229663285, 0.5009369015669936, 0.5034639046945747, 0.5064256118459134, 0.5050711656245209, 0.5074849169143336, 0.5076141841635036, 0.5071154184941395, 0.5142628678110822, 0.515783187936277, 0.5170037910766704, 0.5173103718092152, 0.5229672439078519, 0.5155086380265352, 0.5120058387614138, 0.5117942065393213, 0.5140043333127421, 0.520463119939416, 0.517144497905413, 0.51362568322889, 0.5118216615302955, 0.51702895815173, 0.5170838681336785, 0.5072218065841645, 0.5028335838601259, 0.5029376840342363, 0.5122380622267371, 0.5116294765934762, 0.5126613266709221, 0.5176398317009053, 0.5186465147032916, 0.5188730183788285, 0.5227075654515545, 0.5199288915733769, 0.5180722478087485, 0.5181260138327396, 0.5181385973702695, 0.5242427570301936, 0.5241786953845873, 0.5234442743760281, 0.5233813566883789, 0.5252620235701098, 0.5356034035037145, 0.5359626063022931, 0.5390192619640843, 0.5379553810638351, 0.5380148668776126, 0.538648619585933, 0.5367473614609716, 0.5351069257502649, 0.5356583134856628, 0.5327892669288619, 0.5330889839136632, 0.534898725402044, 0.5354180823146387, 0.5362028374733172, 0.5345166434443201, 0.53385772366094, 0.5319312984609189, 0.5397468192248999]}\n"
     ]
    }
   ],
   "source": [
    "_params=this_trials2.trials[4]['result']['params']\n",
    "trainp=Pool(x_train2,label=y_train)\n",
    "testp=Pool(x_test2,label=y_test)\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "model3,result=fit(params=_params,dtrain=trainp,dtest=testp,n_estimators=501,run_time=run_time)\n",
    "res=model3.eval_metrics(testp,['Logloss','AUC'])\n",
    "print(np.min(res['Logloss']),np.max(res['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0897563676875628 0.5537854712763597\n"
     ]
    }
   ],
   "source": [
    "print(np.min(res['Logloss']),np.max(res['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0897563676875628 81\n"
     ]
    }
   ],
   "source": [
    "xtestp=Pool(x_test2,y_test)\n",
    "res=model3.eval_metrics(xtestp,['Logloss'])\n",
    "#res=model.eval_metrics(testp,['Logloss','AUC'])\n",
    "min_loss=np.min(res['Logloss'])\n",
    "min_loss_arg=np.argmin(res['Logloss'])\n",
    "print(min_loss,min_loss_arg)\n",
    "x=x_test2.copy()\n",
    "loss_gains=[]\n",
    "for col in x_test2.columns:\n",
    "    _res=evaluate_col(col,x,x_test2,y_test,model3)\n",
    "    _loss=_res['Logloss'][(len(_res['Logloss'])-1)]\n",
    "    _min_loss=np.min(_res['Logloss'])\n",
    "    _min_loss_arg=np.argmin(_res['Logloss'])\n",
    "    #print(col,loss_gain,_min_loss,_min_loss_arg,_min_loss/min_loss)\n",
    "    loss_gains.append([col,_min_loss,_min_loss/min_loss,(_min_loss/min_loss)-1])\n",
    "\n",
    "pd_loss_gains=pd.DataFrame(loss_gains,columns=['Feature','Loss','ratio','gains'],index=x_test2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Loss</th>\n",
       "      <th>ratio</th>\n",
       "      <th>gains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Var13_imputed</th>\n",
       "      <td>Var13_imputed</td>\n",
       "      <td>0.091877</td>\n",
       "      <td>1.023626</td>\n",
       "      <td>0.023626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var38_imputed</th>\n",
       "      <td>Var38_imputed</td>\n",
       "      <td>0.091222</td>\n",
       "      <td>1.016324</td>\n",
       "      <td>0.016324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var72_imputed</th>\n",
       "      <td>Var72_imputed</td>\n",
       "      <td>0.090803</td>\n",
       "      <td>1.011662</td>\n",
       "      <td>0.011662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var154_imputed</th>\n",
       "      <td>Var154_imputed</td>\n",
       "      <td>0.090236</td>\n",
       "      <td>1.005345</td>\n",
       "      <td>0.005345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var21</th>\n",
       "      <td>Var21</td>\n",
       "      <td>0.090083</td>\n",
       "      <td>1.003643</td>\n",
       "      <td>0.003643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var140_imputed</th>\n",
       "      <td>Var140_imputed</td>\n",
       "      <td>0.090077</td>\n",
       "      <td>1.003576</td>\n",
       "      <td>0.003576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var24_imputed</th>\n",
       "      <td>Var24_imputed</td>\n",
       "      <td>0.090070</td>\n",
       "      <td>1.003490</td>\n",
       "      <td>0.003490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var103</th>\n",
       "      <td>Var103</td>\n",
       "      <td>0.090024</td>\n",
       "      <td>1.002982</td>\n",
       "      <td>0.002982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var29_imputed</th>\n",
       "      <td>Var29_imputed</td>\n",
       "      <td>0.090003</td>\n",
       "      <td>1.002753</td>\n",
       "      <td>0.002753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var22_imputed</th>\n",
       "      <td>Var22_imputed</td>\n",
       "      <td>0.089990</td>\n",
       "      <td>1.002601</td>\n",
       "      <td>0.002601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var35_imputed</th>\n",
       "      <td>Var35_imputed</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>1.002250</td>\n",
       "      <td>0.002250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var34</th>\n",
       "      <td>Var34</td>\n",
       "      <td>0.089953</td>\n",
       "      <td>1.002192</td>\n",
       "      <td>0.002192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var28</th>\n",
       "      <td>Var28</td>\n",
       "      <td>0.089953</td>\n",
       "      <td>1.002192</td>\n",
       "      <td>0.002192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var99</th>\n",
       "      <td>Var99</td>\n",
       "      <td>0.089939</td>\n",
       "      <td>1.002035</td>\n",
       "      <td>0.002035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var224</th>\n",
       "      <td>Var224</td>\n",
       "      <td>0.089915</td>\n",
       "      <td>1.001764</td>\n",
       "      <td>0.001764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var142_imputed</th>\n",
       "      <td>Var142_imputed</td>\n",
       "      <td>0.089914</td>\n",
       "      <td>1.001754</td>\n",
       "      <td>0.001754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var149</th>\n",
       "      <td>Var149</td>\n",
       "      <td>0.089902</td>\n",
       "      <td>1.001623</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var76_imputed</th>\n",
       "      <td>Var76_imputed</td>\n",
       "      <td>0.089890</td>\n",
       "      <td>1.001485</td>\n",
       "      <td>0.001485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var206</th>\n",
       "      <td>Var206</td>\n",
       "      <td>0.089852</td>\n",
       "      <td>1.001062</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var94_imputed</th>\n",
       "      <td>Var94_imputed</td>\n",
       "      <td>0.089852</td>\n",
       "      <td>1.001060</td>\n",
       "      <td>0.001060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var49_imputed</th>\n",
       "      <td>Var49_imputed</td>\n",
       "      <td>0.089848</td>\n",
       "      <td>1.001019</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var130</th>\n",
       "      <td>Var130</td>\n",
       "      <td>0.089838</td>\n",
       "      <td>1.000914</td>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var162</th>\n",
       "      <td>Var162</td>\n",
       "      <td>0.089823</td>\n",
       "      <td>1.000741</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var189_imputed</th>\n",
       "      <td>Var189_imputed</td>\n",
       "      <td>0.089819</td>\n",
       "      <td>1.000696</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var174_imputed</th>\n",
       "      <td>Var174_imputed</td>\n",
       "      <td>0.089814</td>\n",
       "      <td>1.000641</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var1_imputed</th>\n",
       "      <td>Var1_imputed</td>\n",
       "      <td>0.089811</td>\n",
       "      <td>1.000610</td>\n",
       "      <td>0.000610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var59_imputed</th>\n",
       "      <td>Var59_imputed</td>\n",
       "      <td>0.089805</td>\n",
       "      <td>1.000540</td>\n",
       "      <td>0.000540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var183</th>\n",
       "      <td>Var183</td>\n",
       "      <td>0.089801</td>\n",
       "      <td>1.000493</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var225</th>\n",
       "      <td>Var225</td>\n",
       "      <td>0.089799</td>\n",
       "      <td>1.000472</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var172</th>\n",
       "      <td>Var172</td>\n",
       "      <td>0.089797</td>\n",
       "      <td>1.000449</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var59</th>\n",
       "      <td>Var59</td>\n",
       "      <td>0.089790</td>\n",
       "      <td>1.000380</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var46_imputed</th>\n",
       "      <td>Var46_imputed</td>\n",
       "      <td>0.089787</td>\n",
       "      <td>1.000336</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var132_imputed</th>\n",
       "      <td>Var132_imputed</td>\n",
       "      <td>0.089783</td>\n",
       "      <td>1.000292</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var88</th>\n",
       "      <td>Var88</td>\n",
       "      <td>0.089780</td>\n",
       "      <td>1.000267</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var90_imputed</th>\n",
       "      <td>Var90_imputed</td>\n",
       "      <td>0.089778</td>\n",
       "      <td>1.000242</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var125_imputed</th>\n",
       "      <td>Var125_imputed</td>\n",
       "      <td>0.089777</td>\n",
       "      <td>1.000226</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var10</th>\n",
       "      <td>Var10</td>\n",
       "      <td>0.089775</td>\n",
       "      <td>1.000206</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var123_imputed</th>\n",
       "      <td>Var123_imputed</td>\n",
       "      <td>0.089774</td>\n",
       "      <td>1.000192</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var201</th>\n",
       "      <td>Var201</td>\n",
       "      <td>0.089773</td>\n",
       "      <td>1.000184</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var105_imputed</th>\n",
       "      <td>Var105_imputed</td>\n",
       "      <td>0.089770</td>\n",
       "      <td>1.000150</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var14_imputed</th>\n",
       "      <td>Var14_imputed</td>\n",
       "      <td>0.089770</td>\n",
       "      <td>1.000149</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var139_imputed</th>\n",
       "      <td>Var139_imputed</td>\n",
       "      <td>0.089764</td>\n",
       "      <td>1.000084</td>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var60_imputed</th>\n",
       "      <td>Var60_imputed</td>\n",
       "      <td>0.089764</td>\n",
       "      <td>1.000083</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var143_imputed</th>\n",
       "      <td>Var143_imputed</td>\n",
       "      <td>0.089761</td>\n",
       "      <td>1.000049</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var85_imputed</th>\n",
       "      <td>Var85_imputed</td>\n",
       "      <td>0.089760</td>\n",
       "      <td>1.000045</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var4_imputed</th>\n",
       "      <td>Var4_imputed</td>\n",
       "      <td>0.089759</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var160_imputed</th>\n",
       "      <td>Var160_imputed</td>\n",
       "      <td>0.089756</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var120_imputed</th>\n",
       "      <td>Var120_imputed</td>\n",
       "      <td>0.089755</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>-0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var151_imputed</th>\n",
       "      <td>Var151_imputed</td>\n",
       "      <td>0.089754</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>-0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var108_imputed</th>\n",
       "      <td>Var108_imputed</td>\n",
       "      <td>0.089749</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>-0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var71</th>\n",
       "      <td>Var71</td>\n",
       "      <td>0.089735</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>-0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var165</th>\n",
       "      <td>Var165</td>\n",
       "      <td>0.089727</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>-0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var95_imputed</th>\n",
       "      <td>Var95_imputed</td>\n",
       "      <td>0.089725</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>-0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var81</th>\n",
       "      <td>Var81</td>\n",
       "      <td>0.089686</td>\n",
       "      <td>0.999219</td>\n",
       "      <td>-0.000781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6_imputed</th>\n",
       "      <td>Var6_imputed</td>\n",
       "      <td>0.089673</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>-0.000932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var197</th>\n",
       "      <td>Var197</td>\n",
       "      <td>0.089648</td>\n",
       "      <td>0.998789</td>\n",
       "      <td>-0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var171_imputed</th>\n",
       "      <td>Var171_imputed</td>\n",
       "      <td>0.089645</td>\n",
       "      <td>0.998760</td>\n",
       "      <td>-0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var136</th>\n",
       "      <td>Var136</td>\n",
       "      <td>0.089618</td>\n",
       "      <td>0.998455</td>\n",
       "      <td>-0.001545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var140</th>\n",
       "      <td>Var140</td>\n",
       "      <td>0.089590</td>\n",
       "      <td>0.998147</td>\n",
       "      <td>-0.001853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var5</th>\n",
       "      <td>Var5</td>\n",
       "      <td>0.089520</td>\n",
       "      <td>0.997370</td>\n",
       "      <td>-0.002630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature      Loss     ratio     gains\n",
       "Var13_imputed    Var13_imputed  0.091877  1.023626  0.023626\n",
       "Var38_imputed    Var38_imputed  0.091222  1.016324  0.016324\n",
       "Var72_imputed    Var72_imputed  0.090803  1.011662  0.011662\n",
       "Var154_imputed  Var154_imputed  0.090236  1.005345  0.005345\n",
       "Var21                    Var21  0.090083  1.003643  0.003643\n",
       "Var140_imputed  Var140_imputed  0.090077  1.003576  0.003576\n",
       "Var24_imputed    Var24_imputed  0.090070  1.003490  0.003490\n",
       "Var103                  Var103  0.090024  1.002982  0.002982\n",
       "Var29_imputed    Var29_imputed  0.090003  1.002753  0.002753\n",
       "Var22_imputed    Var22_imputed  0.089990  1.002601  0.002601\n",
       "Var35_imputed    Var35_imputed  0.089958  1.002250  0.002250\n",
       "Var34                    Var34  0.089953  1.002192  0.002192\n",
       "Var28                    Var28  0.089953  1.002192  0.002192\n",
       "Var99                    Var99  0.089939  1.002035  0.002035\n",
       "Var224                  Var224  0.089915  1.001764  0.001764\n",
       "Var142_imputed  Var142_imputed  0.089914  1.001754  0.001754\n",
       "Var149                  Var149  0.089902  1.001623  0.001623\n",
       "Var76_imputed    Var76_imputed  0.089890  1.001485  0.001485\n",
       "Var206                  Var206  0.089852  1.001062  0.001062\n",
       "Var94_imputed    Var94_imputed  0.089852  1.001060  0.001060\n",
       "Var49_imputed    Var49_imputed  0.089848  1.001019  0.001019\n",
       "Var130                  Var130  0.089838  1.000914  0.000914\n",
       "Var162                  Var162  0.089823  1.000741  0.000741\n",
       "Var189_imputed  Var189_imputed  0.089819  1.000696  0.000696\n",
       "Var174_imputed  Var174_imputed  0.089814  1.000641  0.000641\n",
       "Var1_imputed      Var1_imputed  0.089811  1.000610  0.000610\n",
       "Var59_imputed    Var59_imputed  0.089805  1.000540  0.000540\n",
       "Var183                  Var183  0.089801  1.000493  0.000493\n",
       "Var225                  Var225  0.089799  1.000472  0.000472\n",
       "Var172                  Var172  0.089797  1.000449  0.000449\n",
       "...                        ...       ...       ...       ...\n",
       "Var59                    Var59  0.089790  1.000380  0.000380\n",
       "Var46_imputed    Var46_imputed  0.089787  1.000336  0.000336\n",
       "Var132_imputed  Var132_imputed  0.089783  1.000292  0.000292\n",
       "Var88                    Var88  0.089780  1.000267  0.000267\n",
       "Var90_imputed    Var90_imputed  0.089778  1.000242  0.000242\n",
       "Var125_imputed  Var125_imputed  0.089777  1.000226  0.000226\n",
       "Var10                    Var10  0.089775  1.000206  0.000206\n",
       "Var123_imputed  Var123_imputed  0.089774  1.000192  0.000192\n",
       "Var201                  Var201  0.089773  1.000184  0.000184\n",
       "Var105_imputed  Var105_imputed  0.089770  1.000150  0.000150\n",
       "Var14_imputed    Var14_imputed  0.089770  1.000149  0.000149\n",
       "Var139_imputed  Var139_imputed  0.089764  1.000084  0.000084\n",
       "Var60_imputed    Var60_imputed  0.089764  1.000083  0.000083\n",
       "Var143_imputed  Var143_imputed  0.089761  1.000049  0.000049\n",
       "Var85_imputed    Var85_imputed  0.089760  1.000045  0.000045\n",
       "Var4_imputed      Var4_imputed  0.089759  1.000033  0.000033\n",
       "Var160_imputed  Var160_imputed  0.089756  0.999997 -0.000003\n",
       "Var120_imputed  Var120_imputed  0.089755  0.999981 -0.000019\n",
       "Var151_imputed  Var151_imputed  0.089754  0.999979 -0.000021\n",
       "Var108_imputed  Var108_imputed  0.089749  0.999921 -0.000079\n",
       "Var71                    Var71  0.089735  0.999765 -0.000235\n",
       "Var165                  Var165  0.089727  0.999674 -0.000326\n",
       "Var95_imputed    Var95_imputed  0.089725  0.999651 -0.000349\n",
       "Var81                    Var81  0.089686  0.999219 -0.000781\n",
       "Var6_imputed      Var6_imputed  0.089673  0.999068 -0.000932\n",
       "Var197                  Var197  0.089648  0.998789 -0.001211\n",
       "Var171_imputed  Var171_imputed  0.089645  0.998760 -0.001240\n",
       "Var136                  Var136  0.089618  0.998455 -0.001545\n",
       "Var140                  Var140  0.089590  0.998147 -0.001853\n",
       "Var5                      Var5  0.089520  0.997370 -0.002630\n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_loss_gains.sort_values(['gains'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model3.predict(x_train2)\n",
    "(pred!=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10611579386124535 0.563275746489765 7\n"
     ]
    }
   ],
   "source": [
    "_params=this_trials2.trials[18]['result']['params']\n",
    "trainp=Pool(x_train2,label=y_train)\n",
    "testp=Pool(x_test2,label=y_test)\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "model4,result=fit(params=_params,dtrain=trainp,dtest=testp,n_estimators=501,run_time=run_time)\n",
    "res=model4.eval_metrics(testp,['Logloss','AUC'])\n",
    "print(np.min(res['Logloss']),np.max(res['AUC']),np.argmax(res['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2903699832023646 0.5269390659354487\n",
      "1 0.18508738975275824 0.5449769950054796\n",
      "2 0.28016044733153844 0.5619727783764491\n",
      "3 0.25770335215685813 0.5378741600488699\n",
      "4 0.0897563676875628 0.5537854712763597\n",
      "5 0.2193472006412915 0.5431603897693552\n",
      "6 0.23698829489269224 0.5477854117905459\n",
      "7 0.21757901931250448 0.5489716961922215\n",
      "8 0.19557604781729732 0.5466666209083484\n",
      "9 0.16063337933856986 0.5592181276153739\n",
      "10 0.18013548128477175 0.5504485459150406\n",
      "11 0.09326206356358023 0.5548196092697202\n",
      "12 0.10225281187206911 0.528367869424063\n",
      "13 0.27360733297277373 0.5514174783048373\n",
      "14 0.30120191250127043 0.5539318978948886\n",
      "15 0.12396593606734678 0.5620036652412951\n",
      "16 0.20127920316890022 0.5486353725527879\n",
      "17 0.11577213817250777 0.5600852477469749\n",
      "18 0.10611579386124535 0.563275746489765\n",
      "19 0.2392740105639319 0.5425415085144791\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    _params=this_trials2.trials[i]['result']['params']\n",
    "    trainp=Pool(x_train2,label=y_train)\n",
    "    testp=Pool(x_test2,label=y_test)\n",
    "    #run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "    model4,result=fit(params=_params,dtrain=trainp,dtest=testp,n_estimators=501,run_time=run_time)\n",
    "    res=model4.eval_metrics(testp,['Logloss','AUC'])\n",
    "    print(i,np.min(res['Logloss']),np.max(res['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.11938809290062177 0.5326531359319482 34\n",
      "1 0.3161315792705446 0.5467592815028862 4\n",
      "2 0.1767237980171387 0.5450810951795899 2\n",
      "3 0.320343109727721 0.5276300165416321 4\n",
      "4 0.14038285418842983 0.5490060149309393 75\n",
      "5 0.1098728607278975 0.5237131044959835 4\n",
      "6 0.1405222520154462 0.5631384715348942 2\n",
      "7 0.1385115570167563 0.5376488003312903 0\n",
      "8 0.20724260455453208 0.5546697507773195 148\n",
      "9 0.12329385249945556 0.5499154615069587 9\n",
      "10 0.236365761226885 0.547839177814537 3\n",
      "11 0.3019953893496173 0.5376488003312903 0\n",
      "12 0.19942217640467844 0.51657023101087 14\n",
      "13 0.18336319634957854 0.55569130523315 123\n",
      "14 0.2684785134756219 0.550041296882257 9\n",
      "15 0.08982621032298467 0.572541805943548 6\n",
      "16 0.1707654111841392 0.5328155779618788 0\n",
      "17 0.2601889243578647 0.5584196449612084 16\n",
      "18 0.12459315178475733 0.541340352659359 4\n",
      "19 0.2566187853034362 0.551082298623361 1\n"
     ]
    }
   ],
   "source": [
    "trainp=Pool(x_train2,label=y_train)\n",
    "testp=Pool(x_test2,label=y_test)\n",
    "for i in range(20):\n",
    "    _params=this_trials.trials[i]['result']['params']\n",
    "    #trainp=Pool(x_train2,label=y_train)\n",
    "    #testp=Pool(x_test2,label=y_test)\n",
    "    #run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "    model4,result=fit(params=_params,dtrain=trainp,dtest=testp,n_estimators=501,run_time=run_time)\n",
    "    res=model4.eval_metrics(testp,['Logloss','AUC'])\n",
    "    print(i,np.min(res['Logloss']),np.max(res['AUC']),np.argmax(res['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Var5', 'Var10', 'Var21', 'Var28', 'Var34', 'Var59', 'Var71', 'Var81',\n",
       "       'Var88', 'Var99', 'Var103', 'Var130', 'Var136', 'Var140', 'Var149',\n",
       "       'Var162', 'Var165', 'Var172', 'Var183', 'Var197', 'Var201', 'Var206',\n",
       "       'Var224', 'Var225', 'Var1_imputed', 'Var4_imputed', 'Var6_imputed',\n",
       "       'Var13_imputed', 'Var14_imputed', 'Var22_imputed', 'Var24_imputed',\n",
       "       'Var29_imputed', 'Var34_imputed', 'Var35_imputed', 'Var38_imputed',\n",
       "       'Var46_imputed', 'Var49_imputed', 'Var59_imputed', 'Var60_imputed',\n",
       "       'Var72_imputed', 'Var76_imputed', 'Var85_imputed', 'Var90_imputed',\n",
       "       'Var94_imputed', 'Var95_imputed', 'Var105_imputed', 'Var108_imputed',\n",
       "       'Var120_imputed', 'Var123_imputed', 'Var125_imputed', 'Var132_imputed',\n",
       "       'Var139_imputed', 'Var140_imputed', 'Var142_imputed', 'Var143_imputed',\n",
       "       'Var151_imputed', 'Var154_imputed', 'Var160_imputed', 'Var171_imputed',\n",
       "       'Var174_imputed', 'Var189_imputed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_temperature': 0.6360830384671156,\n",
       " 'border_count': 128,\n",
       " 'class_weights': (1, 15.39192339060007),\n",
       " 'depth': 8,\n",
       " 'l2_leaf_reg': 1.6561896117976493,\n",
       " 'leaf_estimation_iterations': 1,\n",
       " 'leaf_estimation_method': 'Newton',\n",
       " 'learning_rate': 0.10383569528314504,\n",
       " 'max_ctr_complexity': 3.0,\n",
       " 'one_hot_max_size': 0,\n",
       " 'random_strength': 1,\n",
       " 'iterations': 501,\n",
       " 'eval_metric': 'Logloss',\n",
       " 'logging_level': 'Silent',\n",
       " 'random_seed': 0,\n",
       " 'rsm': 1,\n",
       " 'thread_count': 8,\n",
       " 'fold_len_multiplier': 2,\n",
       " 'train_dir': './cv_run/201809141356/0.0',\n",
       " 'calc_feature_importance': True,\n",
       " 'od_type': 'Iter',\n",
       " 'od_wait': 30}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_trials.trials[14]['result']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_temperature': 0.7242644466027569,\n",
       " 'border_count': 64,\n",
       " 'class_weights': (1, 3.899492351596896),\n",
       " 'depth': 6,\n",
       " 'l2_leaf_reg': 8.197734346168204,\n",
       " 'leaf_estimation_iterations': 5,\n",
       " 'leaf_estimation_method': 'Newton',\n",
       " 'learning_rate': 0.7250153244222955,\n",
       " 'max_ctr_complexity': 3.0,\n",
       " 'one_hot_max_size': 25,\n",
       " 'random_strength': 1,\n",
       " 'iterations': 501,\n",
       " 'eval_metric': 'Logloss',\n",
       " 'logging_level': 'Silent',\n",
       " 'random_seed': 0,\n",
       " 'rsm': 1,\n",
       " 'thread_count': 8,\n",
       " 'fold_len_multiplier': 2,\n",
       " 'train_dir': './cv_run/201809141412/0.0',\n",
       " 'calc_feature_importance': True,\n",
       " 'od_type': 'Iter',\n",
       " 'od_wait': 30}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_trials2.trials[17]['result']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4]\teval_time=265.51 sec\tcurrent_AUC=0.478115\tmin_AUC=0.445421\n",
      "[2/4]\teval_time=123.29 sec\tcurrent_AUC=0.493398\tmin_AUC=0.445421\n",
      "[3/4]\teval_time=57.36 sec\tcurrent_AUC=0.468019\tmin_AUC=0.445421\n",
      "[4/4]\teval_time=421.49 sec\tcurrent_AUC=0.472625\tmin_AUC=0.445421\n",
      "[5/4]\teval_time=91.08 sec\tcurrent_AUC=0.470613\tmin_AUC=0.445421\n",
      "[6/4]\teval_time=200.46 sec\tcurrent_AUC=0.474690\tmin_AUC=0.445421\n",
      "[7/4]\teval_time=71.52 sec\tcurrent_AUC=0.448925\tmin_AUC=0.445421\n",
      "[8/4]\teval_time=177.51 sec\tcurrent_AUC=0.480839\tmin_AUC=0.445421\n"
     ]
    }
   ],
   "source": [
    "n_estimators=401\n",
    "\n",
    "metric=\"AUC\"\n",
    "to_run=8\n",
    "\n",
    "run_time_saved='201808031235'\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "\n",
    "with open('./cv_run/'+run_time_saved+'/trails.pickle','rb') as f:\n",
    "    this_trials=pickle.load(f)\n",
    "    max_evals=to_run+len(this_trials.trials)\n",
    "    #print(max_evals)\n",
    "#this_trials=this_trials\n",
    "#max_evals = max_evals + to_run # last run + # additional run\n",
    "hyperopt_evals=len(this_trials.trials)\n",
    "\n",
    "hyperopt_eval_num, best_loss = 0, this_trials.best_trial['result']['loss']\n",
    "args=space2 #or pyll.stochastic.sample(space)\n",
    "_ = fmin(fn=lambda args: run_hyper_test2(args,data,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n",
    "\n",
    "with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=data['numeric_columns'] + data['category_columns']\n",
    "features=[]\n",
    "klas=1\n",
    "for _dir in os.listdir(\"cv_run\"):\n",
    "    if not os.path.isdir(\"cv_run/\"+_dir):\n",
    "        continue\n",
    "    for file in os.listdir(\"cv_run/\"+_dir):\n",
    "        #print(file)\n",
    "        if file.startswith('feature'):\n",
    "            features.append(pd.read_pickle(\"cv_run/\"+_dir+\"/\"+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix=[]\n",
    "for f in features:\n",
    "    matrix=[]\n",
    "    for col in cols:\n",
    "        try:\n",
    "            matrix.append(f[col])\n",
    "        except KeyError:\n",
    "            matrix.append(0)\n",
    "    feature_matrix.append(matrix)\n",
    "\n",
    "loss=[]\n",
    "for t in this_trials.trials:\n",
    "    loss.append(t['result']['losses'])\n",
    "        \n",
    "loss=np.squeeze(np.array(loss))\n",
    "loss=loss.flatten()\n",
    "y=[]\n",
    "_=[y.append(i) for i in range(len(loss))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x265152dd9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features=np.squeeze(np.array(feature_matrix))\n",
    "plt.figure(figsize=(30,5))\n",
    "ax=sns.heatmap(features[10:15,:],xticklabels=cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26514b5dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(y,1-loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26514f334a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.Figure(figsize=(3,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.barplot(y,1-loss,color='blue')\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu3",
   "language": "python",
   "name": "tensorflow-gpu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
